{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows details of LLM inference using huggingface API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "#model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model() directly\n",
    "- This generates an CausalLMOutputWithCrossAttentions object with many attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: {'input_ids': tensor([[4919,  389,  345]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}\n",
      "outputs_obj type: <class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>\n",
      "logits shape: torch.Size([1, 3, 50257])\n",
      "Greedy: next_token_id: 1016, next_token:  going\n",
      "Topk: k: 5, next_token_ids: tensor([1016, 1804, 1701,   30, 4203], device='cuda:0'), next_tokens: [' going', ' doing', '?\"', '?', ' feeling']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"how are you\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "print(f\"input: {inputs}\")\n",
    " \n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs_obj = model(**inputs)\n",
    "print(\"outputs_obj type:\", type(outputs_obj)) # object transformers.modeling_outputs.CausalLMOutputWithCrossAttentions\n",
    "\n",
    "# Get output logits\n",
    "logits = outputs_obj.logits\n",
    "print(\"logits shape:\", logits.shape) # batchsize, numtokens, vocabsize\n",
    "\n",
    "# Last token logit\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Generate greedy next token\n",
    "next_token_id = last_token_logits.argmax()\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f\"Greedy: next_token_id: {next_token_id}, next_token: {next_token}\")\n",
    "\n",
    "# Generate topk next tokens\n",
    "k = 5\n",
    "topk = torch.topk(last_token_logits, k=k)\n",
    "next_token_ids = topk.indices\n",
    "next_tokens = [tokenizer.decode(next_token_id) for next_token_id in next_token_ids]\n",
    "print(f\"Topk: k: {k}, next_token_ids: {next_token_ids}, next_tokens: {next_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model.generate() function - return Tensor\n",
    "- By default, this generates Tensor of next tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 1,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"temperature\": 0.05\n",
      "}\n",
      "\n",
      "generation_config.max_new_tokens: 1\n",
      "generation_config.output_scores: False\n",
      "generation_config.return_dict_in_generate: False\n",
      "\n",
      "inputs: {'input_ids': tensor([[4919,  389,  345]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}\n",
      "outputs type: <class 'torch.Tensor'>\n",
      "outputs.shape: torch.Size([1, 4]), outputs: tensor([[4919,  389,  345, 1804]], device='cuda:0')\n",
      "\n",
      "Full response (including input) using tokenizer.decode(tensor([4919,  389,  345, 1804], device='cuda:0')) with input shape: torch.Size([4]):\n",
      "how are you doing\n",
      "Only new tokens response using tokenizer.decode(tensor([1804], device='cuda:0')) with input shape: torch.Size([1]):\n",
      " doing\n",
      "\n",
      "Full response (including input) using tokenizer.batch_decode(tensor([[4919,  389,  345, 1804]], device='cuda:0')) with input shape: torch.Size([1, 4]):\n",
      "['how are you doing']\n",
      "Only new tokens response using tokenizer.batch_decode(tensor([1804], device='cuda:0')) with input shape: torch.Size([1]):\n",
      "[' doing']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize & generate output\n",
    "def tokenize_generate_response(tokenizer, model, generation_config, text, apply_chat_template=False):\n",
    "\n",
    "    # Tokenize input\n",
    "    if apply_chat_template:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            text,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    print(f\"inputs: {inputs}\")\n",
    "\n",
    "    # Output a tensor directly\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    print(\"outputs type:\", type(outputs)) # Tensor\n",
    "    print(f\"outputs.shape: {outputs.shape}, outputs: {outputs}\")  # batchsize, totalnumtokens\n",
    "    print()\n",
    "\n",
    "    # Use tokenizer.decode to decode 1 sample at a time\n",
    "    decode_arg = outputs[0]  # (totalnumtokens, )\n",
    "    full_response = tokenizer.decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Full response (including input) using tokenizer.decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(full_response)\n",
    "    decode_arg = outputs[0][len(inputs.input_ids[0]):]\n",
    "    response = tokenizer.decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Only new tokens response using tokenizer.decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    # Use tokenizer.batch_decode to decode many samples at a time\n",
    "    decode_arg = outputs\n",
    "    full_response = tokenizer.batch_decode(outputs, skip_special_tokens=True) #[0]\n",
    "    print(f\"Full response (including input) using tokenizer.batch_decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(full_response)\n",
    "    decode_arg = torch.tensor([output[len(input_ids):] for input_ids, output in zip(inputs.input_ids, outputs)]).to(model.device) # Select only new token ixs\n",
    "    response = tokenizer.batch_decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Only new tokens response using tokenizer.batch_decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig(\n",
    "    #max_length=256,\n",
    "    max_new_tokens=1,\n",
    "    temperature=0.05,\n",
    "    do_sample=True,\n",
    "    #do_sample=False,\n",
    "    use_cache=True,\n",
    "    skip_special_tokens=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(generation_config)\n",
    "for attr in [\"max_new_tokens\", \"output_scores\", \"return_dict_in_generate\"]:\n",
    "    attr_name = f\"generation_config.{attr}\"\n",
    "    att_val = eval(attr_name)\n",
    "    print(f\"{attr_name}: {att_val}\")\n",
    "print()\n",
    "\n",
    "# Input\n",
    "text = \"how are you\"\n",
    "tokenize_generate_response(tokenizer, model, generation_config, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model.generate() function - return scores, etc. \n",
    "- if generation_config.return_dict_in_generate = False, it generates GenerateDecoderOnlyOutput object with attributes\n",
    "- if generation_config.output_scores = True, it generate scores attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 1,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"temperature\": 0.05\n",
      "}\n",
      "\n",
      "generation_config.max_new_tokens: 1\n",
      "generation_config.output_scores: False\n",
      "generation_config.return_dict_in_generate: False\n",
      "\n",
      "inputs: {'input_ids': tensor([[4919,  389,  345]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Generating object with scores ...\n",
      "\n",
      "outputs_obj type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n",
      "sequences.shape: torch.Size([1, 4])\n",
      "sequences[0].shape: torch.Size([4])\n",
      "scores tuple len: 1\n",
      "scores[0].shape: torch.Size([1, 50257])\n",
      "\n",
      "scores_tensor.shape: torch.Size([1, 1, 50257])\n",
      "probs.shape: torch.Size([1, 1, 50257]), probs:tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "max_probs_obj: torch.return_types.max(\n",
      "values=tensor([[0.9719]], device='cuda:0'),\n",
      "indices=tensor([[1016]], device='cuda:0'))\n",
      "max_probs.shape: torch.Size([1, 1]), max_probs: tensor([[0.9719]], device='cuda:0')\n",
      "\n",
      "Decoding from generate object outputs ... \n",
      "\n",
      "Full response (including input) using tokenizer.decode(tensor([4919,  389,  345, 1016], device='cuda:0')) with input shape: torch.Size([4]):\n",
      "how are you going\n",
      "Only new tokens response using tokenizer.decode(tensor([1016], device='cuda:0')) with input shape: torch.Size([1]):\n",
      " going\n",
      "\n",
      "Full response (including input) using tokenizer.batch_decode(tensor([[4919,  389,  345, 1016]], device='cuda:0')) with input shape: torch.Size([1, 4]):\n",
      "['how are you going']\n",
      "Only new tokens response using tokenizer.batch_decode(tensor([1016], device='cuda:0')) with input shape: torch.Size([1]):\n",
      "[' going']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to tokenize & generate output\n",
    "def tokenize_generate_response(tokenizer, model, generation_config, text, apply_chat_template=False):\n",
    "\n",
    "    # Tokenize input\n",
    "    if apply_chat_template:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            text,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print()\n",
    "\n",
    "    # Output an object with more information, like scores \n",
    "    print(\"Generating object with scores ...\\n\")\n",
    "    generation_config.output_scores= True\n",
    "    generation_config.return_dict_in_generate=True\n",
    "    outputs_obj = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    print(\"outputs_obj type:\", type(outputs_obj)) # transformers.generation.utils.GenerateDecoderOnlyOutput\n",
    "    scores = outputs_obj.scores\n",
    "    sequences = outputs_obj.sequences\n",
    "    print(f\"sequences.shape: {sequences.shape}\") # batchsize, totalnumtokens\n",
    "    print(f\"sequences[0].shape: {sequences[0].shape}\") # totalnumtokens,\n",
    "    print(f\"scores tuple len: {len(scores)}\")  # totalnumtokens\n",
    "    print(f\"scores[0].shape: {scores[0].shape}\")  # batchsize, vocabsize\n",
    "    print()\n",
    "\n",
    "    # Get scores\n",
    "    scores_tensor = torch.stack(scores, dim=1) # batchsize, totalnumtokens, vocabsize\n",
    "    probs = F.softmax(scores_tensor, dim=-1) # batchsize, totalnumtokens, vocabsize\n",
    "    max_probs_obj = torch.max(probs, dim=-1) # (values, indices) with sizes (batchsize, totalnumtokens) \n",
    "    max_probs = max_probs_obj[0] # batchsize, totalnumtokens\n",
    "    print(f\"scores_tensor.shape: {scores_tensor.shape}\")\n",
    "    print(f\"probs.shape: {probs.shape}, probs:{probs}\")\n",
    "    print(f\"max_probs_obj: {max_probs_obj}\")\n",
    "    print(f\"max_probs.shape: {max_probs.shape}, max_probs: {max_probs}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Decoding from generate object outputs ... \\n\")\n",
    "\n",
    "    # Use tokenizer.decode to decode 1 sample at a time\n",
    "    decode_arg = sequences[0]  # (totalnumtokens, )\n",
    "    full_response = tokenizer.decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Full response (including input) using tokenizer.decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(full_response)\n",
    "    decode_arg = sequences[0][len(inputs.input_ids[0]):]\n",
    "    response = tokenizer.decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Only new tokens response using tokenizer.decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    # Use tokenizer.batch_decode to decode many samples at a time\n",
    "    decode_arg = sequences\n",
    "    full_response = tokenizer.batch_decode(sequences, skip_special_tokens=True) #[0]\n",
    "    print(f\"Full response (including input) using tokenizer.batch_decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(full_response)\n",
    "    decode_arg = torch.tensor([output[len(input_ids):] for input_ids, output in zip(inputs.input_ids, sequences)]).to(model.device) # Select only new token ixs\n",
    "    response = tokenizer.batch_decode(decode_arg, skip_special_tokens=True) #[0]\n",
    "    print(f\"Only new tokens response using tokenizer.batch_decode({decode_arg}) with input shape: {decode_arg.shape}:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig(\n",
    "    #max_length=256,\n",
    "    max_new_tokens=1,\n",
    "    temperature=0.05,\n",
    "    do_sample=True,\n",
    "    #do_sample=False,\n",
    "    use_cache=True,\n",
    "    skip_special_tokens=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(generation_config)\n",
    "for attr in [\"max_new_tokens\", \"output_scores\", \"return_dict_in_generate\"]:\n",
    "    attr_name = f\"generation_config.{attr}\"\n",
    "    att_val = eval(attr_name)\n",
    "    print(f\"{attr_name}: {att_val}\")\n",
    "print()\n",
    "\n",
    "# Input\n",
    "text = \"how are you\"\n",
    "tokenize_generate_response(tokenizer, model, generation_config, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

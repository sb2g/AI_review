{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:07:59.660487Z",
     "iopub.status.busy": "2024-09-27T21:07:59.660184Z",
     "iopub.status.idle": "2024-09-27T21:10:06.325603Z",
     "shell.execute_reply": "2024-09-27T21:10:06.324332Z",
     "shell.execute_reply.started": "2024-09-27T21:07:59.660461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%capture\\n%pip install -U transformers \\n%pip install -U datasets \\n%pip install -U accelerate \\n%pip install -U peft \\n%pip install -U trl \\n%pip install -U bitsandbytes \\n%pip install -U wandb\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%capture\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:06.328308Z",
     "iopub.status.busy": "2024-09-27T21:10:06.327898Z",
     "iopub.status.idle": "2024-09-27T21:10:20.481499Z",
     "shell.execute_reply": "2024-09-27T21:10:20.480549Z",
     "shell.execute_reply.started": "2024-09-27T21:10:06.328271Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch #, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:20.483420Z",
     "iopub.status.busy": "2024-09-27T21:10:20.482719Z",
     "iopub.status.idle": "2024-09-27T21:10:20.791138Z",
     "shell.execute_reply": "2024-09-27T21:10:20.790252Z",
     "shell.execute_reply.started": "2024-09-27T21:10:20.483383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:19:54.199330Z",
     "iopub.status.busy": "2024-09-27T21:19:54.198447Z",
     "iopub.status.idle": "2024-09-27T21:19:58.529114Z",
     "shell.execute_reply": "2024-09-27T21:19:58.528089Z",
     "shell.execute_reply.started": "2024-09-27T21:19:54.199296Z"
    },
    "id": "na9CAoHC5gM9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:34:05.649291Z",
     "iopub.status.busy": "2024-09-27T21:34:05.648875Z",
     "iopub.status.idle": "2024-09-27T21:34:05.654251Z",
     "shell.execute_reply": "2024-09-27T21:34:05.652981Z",
     "shell.execute_reply.started": "2024-09-27T21:34:05.649255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "new_model = \"output_dir/llama-3.2-3b-it-Ecommerce-ChatBot\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:14:05.900281Z",
     "iopub.status.busy": "2024-09-27T21:14:05.899852Z",
     "iopub.status.idle": "2024-09-27T21:14:06.397447Z",
     "shell.execute_reply": "2024-09-27T21:14:06.396481Z",
     "shell.execute_reply.started": "2024-09-27T21:14:05.900222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Set torch dtype and attention implementation\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     !pip install -qqq flash-attn\n",
    "#     torch_dtype = torch.bfloat16\n",
    "#     attn_implementation = \"flash_attention_2\"\n",
    "# else:\n",
    "#     torch_dtype = torch.float16\n",
    "#     attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:14:14.553186Z",
     "iopub.status.busy": "2024-09-27T21:14:14.552280Z",
     "iopub.status.idle": "2024-09-27T21:14:59.093678Z",
     "shell.execute_reply": "2024-09-27T21:14:59.092505Z",
     "shell.execute_reply.started": "2024-09-27T21:14:14.553130Z"
    },
    "id": "StJKGiDDHzdk",
    "outputId": "871214ba-6c30-4ecf-ac68-550f296b7ef6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16\n",
    "attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:14:59.095982Z",
     "iopub.status.busy": "2024-09-27T21:14:59.095627Z",
     "iopub.status.idle": "2024-09-27T21:14:59.590090Z",
     "shell.execute_reply": "2024-09-27T21:14:59.589382Z",
     "shell.execute_reply.started": "2024-09-27T21:14:59.095946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "['v_proj', 'down_proj', 'up_proj', 'k_proj', 'q_proj', 'gate_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 26872\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample instruction: I need to cancel purchase {{Order Number}}\n",
      "Sample response: I understood that you need assistance with canceling your purchase with the order number {{Order Num\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Mar 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI need to cancel purchase {{Order Number}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:\\n\\n1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.\\n2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.\\n3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.\\n4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.\\n5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.\\n\\nIf you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction.<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "sample_instruction = dataset['instruction'][3]\n",
    "sample_response = dataset['response'][3]\n",
    "print(\"Sample instruction:\", sample_instruction[:100])\n",
    "print(\"Sample response:\", sample_response[:100])\n",
    "\n",
    "message = [ #{\"role\": \"system\", \"content\": instruction },\n",
    "            {\"role\": \"user\", \"content\": sample_instruction},\n",
    "            {\"role\": \"assistant\", \"content\": sample_response}]\n",
    "    \n",
    "tokenized_chat = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "tokenized_chat\n",
    "#print(tokenizer.decode(tokenized_chat[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:15:00.330556Z",
     "iopub.status.busy": "2024-09-27T21:15:00.330315Z",
     "iopub.status.idle": "2024-09-27T21:15:01.476477Z",
     "shell.execute_reply": "2024-09-27T21:15:01.475609Z",
     "shell.execute_reply.started": "2024-09-27T21:15:00.330534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "#tokenizer.chat_template = None # sbujimal added\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer) # sbujimal commented out\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Mar 2025\\n\\nYou are a top-rated customer service agent named John. \\n    Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ncould you tell me about the options for shipping?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOf course, I'd be delighted to provide you with information about our shipping options! Here are the various choices we offer:\\n\\n1. Standard Shipping: This option typically arrives within {{Date Range}} business days, catering to non-urgent items and ensuring a cost-effective delivery.\\n\\n2. Expedited Shipping: If you're looking for a faster option, choose expedited shipping. Your items will reach you within {{Date Range}} business days, offering a balance between speed and affordability.\\n\\n3. Overnight Shipping: For urgent needs, we have overnight shipping. This ensures your items are delivered on the next business day, offering the highest level of speed and convenience.\\n\\n4. In-Store Pickup: If you prefer a more hands-on approach, you can opt for in-store pickup. This option allows you to collect your items personally from one of our {{Store Location}}, offering added flexibility and convenience.\\n\\nRemember that specific delivery times may vary based on your location and other factors. If you have any further questions or need assistance with any aspect of our shipping options, please feel free to ask. We're here to ensure your satisfaction!<|eot_id|>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.171305Z",
     "iopub.status.busy": "2024-09-27T21:21:29.170953Z",
     "iopub.status.idle": "2024-09-27T21:21:29.189225Z",
     "shell.execute_reply": "2024-09-27T21:21:29.188383Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.171272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train & test\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.192045Z",
     "iopub.status.busy": "2024-09-27T21:21:29.191332Z",
     "iopub.status.idle": "2024-09-27T21:21:29.228569Z",
     "shell.execute_reply": "2024-09-27T21:21:29.227705Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.192016Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1, #2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.229887Z",
     "iopub.status.busy": "2024-09-27T21:21:29.229613Z",
     "iopub.status.idle": "2024-09-27T21:21:30.281033Z",
     "shell.execute_reply": "2024-09-27T21:21:30.280293Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.229863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_230701/977504036.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer( #Trainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 900/900 [00:00<00:00, 12300.21 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 900/900 [00:00<00:00, 21473.77 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 900/900 [00:00<00:00, 2006.42 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 900/900 [00:00<00:00, 4931.20 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 100/100 [00:00<00:00, 10174.67 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 100/100 [00:00<00:00, 12591.73 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 100/100 [00:00<00:00, 1893.57 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 100/100 [00:00<00:00, 4254.85 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer( #Trainer( \n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:30.282592Z",
     "iopub.status.busy": "2024-09-27T21:21:30.282299Z",
     "iopub.status.idle": "2024-09-27T21:29:33.791645Z",
     "shell.execute_reply": "2024-09-27T21:29:33.790553Z",
     "shell.execute_reply.started": "2024-09-27T21:21:30.282566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.345800</td>\n",
       "      <td>1.334226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.078900</td>\n",
       "      <td>1.081131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>1.015295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.960300</td>\n",
       "      <td>0.967429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=1.2440583318994756, metrics={'train_runtime': 17.5908, 'train_samples_per_second': 51.163, 'train_steps_per_second': 3.24, 'total_flos': 1124788478803968.0, 'train_loss': 1.2440583318994756})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "model.config.use_cache = False\n",
    "# Pad token was not set by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:29:33.793271Z",
     "iopub.status.busy": "2024-09-27T21:29:33.792969Z",
     "iopub.status.idle": "2024-09-27T21:29:35.162222Z",
     "shell.execute_reply": "2024-09-27T21:29:35.161478Z",
     "shell.execute_reply.started": "2024-09-27T21:29:33.793223Z"
    },
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:38:37.936614Z",
     "iopub.status.busy": "2024-09-27T21:38:37.935932Z",
     "iopub.status.idle": "2024-09-27T21:38:58.553883Z",
     "shell.execute_reply": "2024-09-27T21:38:58.552887Z",
     "shell.execute_reply.started": "2024-09-27T21:38:37.936581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thank you for contacting us about the repeated purchase of the same item. I understand your concern and I'm here to assist you with canceling your order with the order number {{Order Number}}. To proceed, could you please provide me with the order details or any other relevant information? Once I have this information, I will be able to cancel the order for you and ensure that you receive a full refund. Thank you for bringing this to my attention, and I appreciate your cooperation in resolving this matter. I'm here to help and I'm committed to providing you with the best possible service. Let me know if there's anything else I can assist you with.\n"
     ]
    }
   ],
   "source": [
    "## Run inference\n",
    "\n",
    "# Tokenize input\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "\n",
    "# Llama example for this notebook\n",
    "https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "https://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392\n",
    "\n",
    "# Quantization training\n",
    "https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.\n",
    "\n",
    "# Data\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "# Training/Lora/PEFT\n",
    "https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods\n",
    "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint\n",
    "\n",
    "# Generation\n",
    "https://huggingface.co/docs/transformers/main/en/llm_tutorial\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate\n",
    "\n",
    "# Caching & optimization\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_optims\n",
    "https://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation\n",
    "\n",
    "# HF notebooks\n",
    "https://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

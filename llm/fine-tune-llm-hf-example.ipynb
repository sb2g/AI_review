{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to finetune LLM that has not be finetuned yet on instruction dataset using hugging face trainer. \n",
    "- So there will be new tokens added into the tokenizer before finetuning. \n",
    "- Llama 3.2 1B model was used as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Conda env: \n",
    "# Setup: conda env create -f environment_mlenv2\n",
    "# Activate: conda activate mlenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:06.328308Z",
     "iopub.status.busy": "2024-09-27T21:10:06.327898Z",
     "iopub.status.idle": "2024-09-27T21:10:20.481499Z",
     "shell.execute_reply": "2024-09-27T21:10:20.480549Z",
     "shell.execute_reply.started": "2024-09-27T21:10:06.328271Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch #, wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)\n",
    "\n",
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:34:05.649291Z",
     "iopub.status.busy": "2024-09-27T21:34:05.648875Z",
     "iopub.status.idle": "2024-09-27T21:34:05.654251Z",
     "shell.execute_reply": "2024-09-27T21:34:05.652981Z",
     "shell.execute_reply.started": "2024-09-27T21:34:05.649255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B\"\n",
    "new_model = \"output_dir/Ecommerce-ChatBot\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Load slow tokenizer, fast tokenizer sometimes ignores added tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 26872\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Check a train sample:\n",
      "<|im_start|>system\n",
      "    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\n",
      "    <|im_start|>user\n",
      "    could you tell me about the options for shipping?<|im_end|>\n",
      "    <|im_start|>assistant\n",
      "    Of course, I'd be delighted to provide you with information about our shipping options! Here are the various choices we offer:\n",
      "\n",
      "1. Standard Shipping: This option typically arrives within {{Date Range}} business days, catering to non-urgent items and ensuring a cost-effective delivery.\n",
      "\n",
      "2. Expedited Shipping: If you're looking for a faster option, choose expedited shipping. Your items will reach you within {{Date Range}} business days, offering a balance between speed and affordability.\n",
      "\n",
      "3. Overnight Shipping: For urgent needs, we have overnight shipping. This ensures your items are delivered on the next business day, offering the highest level of speed and convenience.\n",
      "\n",
      "4. In-Store Pickup: If you prefer a more hands-on approach, you can opt for in-store pickup. This option allows you to collect your items personally from one of our {{Store Location}}, offering added flexibility and convenience.\n",
      "\n",
      "Remember that specific delivery times may vary based on your location and other factors. If you have any further questions or need assistance with any aspect of our shipping options, please feel free to ask. We're here to ensure your satisfaction!<|im_end|>\n",
      "Check a inference sample:\n",
      "{'instruction': 'I bought the same item twice, cancel order {{Order Number}}', 'text': '<|im_start|>system\\n    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\\n    <|im_start|>user\\n    I bought the same item twice, cancel order {{Order Number}}<|im_end|>\\n    <|im_start|>assistant'}\n"
     ]
    }
   ],
   "source": [
    "# Chat template for training\n",
    "def format_chat_template(row) -> str:\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"\"\"\n",
    "    prompt=f\"\"\"<|im_start|>system\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {row[\"instruction\"]}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {row[\"response\"]}<|im_end|>\"\"\"\n",
    "\n",
    "    row['text'] = prompt\n",
    "    return row\n",
    "\n",
    "# Chat template for inference\n",
    "def format_chat_template_inference(row) -> str:\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"\"\"\n",
    "    prompt=f\"\"\"<|im_start|>system\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {row[\"instruction\"]}<|im_end|>\n",
    "    <|im_start|>assistant\"\"\"\n",
    "\n",
    "    row['text'] = prompt\n",
    "    return row\n",
    "\n",
    "# Select subset of data for train/test & check template \n",
    "if 1:\n",
    "\n",
    "    dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "    dataset = dataset.map(format_chat_template, num_proc= 4)\n",
    "    print(dataset)\n",
    "    print(\"Check a train sample:\")\n",
    "    print(dataset['text'][3])\n",
    "\n",
    "    print(\"Check a inference sample:\")\n",
    "    test_sample = {\"instruction\": \"I bought the same item twice, cancel order {{Order Number}}\"}\n",
    "    test_prompt = format_chat_template_inference(test_sample)\n",
    "    print(test_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize without truncation\n",
    "def tokenize(element):\n",
    "    return tokenizer(element['text'])\n",
    "\n",
    "# Tokenize with truncation\n",
    "def tokenize_trunc(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=int(1.5*max_length),\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "# Tokenize with truncation for inference\n",
    "def tokenize_trunc_inference(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=int(1.5*max_length),\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "if 1:\n",
    "\n",
    "    # Add tokens <|im_start|> and <|im_end|>, latter is special eos token\n",
    "    tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "    tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "    #tokenizer.add_special_tokens(dict(pad_token=\"</s>\"))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Get max length\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize, \n",
    "        num_proc=4\n",
    "    )\n",
    "    max_length = max([len(x['input_ids']) for x in tokenized_dataset])\n",
    "    print(max_length)\n",
    "\n",
    "    # Tokenize with max length\n",
    "    dataset = dataset.map(\n",
    "        tokenize_trunc,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=[\"text\"]    \n",
    "    )\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train & test\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "# Transforms list of dicts [ {input_ids: [123, ..]}, {.. ] \n",
    "# into dict of lists (pytorch tensors) { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "# Label shifting should be handled inside the HF model forward function, so they dont need to be shifted here & can be kept same as inputs\n",
    "def collate(tokenized_batch_data):\n",
    "\n",
    "    tokenlist = [e[\"input_ids\"] for e in tokenized_batch_data]\n",
    "    tokens_maxlen = max([len(t) for t in tokenlist])  # length of longest input\n",
    "\n",
    "    input_ids, labels, attention_masks = [], [], []\n",
    "    for tokens in tokenlist:\n",
    "        # Num of pad tokens to add\n",
    "        pad_len = tokens_maxlen-len(tokens)\n",
    "        # Pad\n",
    "        input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )\n",
    "        labels.append( tokens + [-100]*pad_len )\n",
    "        attention_masks.append( [1]*len(tokens) + [0]*pad_len )\n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "        \"attention_mask\": torch.tensor(attention_masks)\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "print(torch_dtype, attn_implementation)\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj', 'k_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "# Update model config based on tokenizer update\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            #print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:15:00.330556Z",
     "iopub.status.busy": "2024-09-27T21:15:00.330315Z",
     "iopub.status.idle": "2024-09-27T21:15:01.476477Z",
     "shell.execute_reply": "2024-09-27T21:15:01.475609Z",
     "shell.execute_reply.started": "2024-09-27T21:15:00.330534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.192045Z",
     "iopub.status.busy": "2024-09-27T21:21:29.191332Z",
     "iopub.status.idle": "2024-09-27T21:21:29.228569Z",
     "shell.execute_reply": "2024-09-27T21:21:29.227705Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.192016Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1, #2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.229887Z",
     "iopub.status.busy": "2024-09-27T21:21:29.229613Z",
     "iopub.status.idle": "2024-09-27T21:21:30.281033Z",
     "shell.execute_reply": "2024-09-27T21:21:30.280293Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.229863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95556/3763218028.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = Trainer( \n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:30.282592Z",
     "iopub.status.busy": "2024-09-27T21:21:30.282299Z",
     "iopub.status.idle": "2024-09-27T21:29:33.791645Z",
     "shell.execute_reply": "2024-09-27T21:29:33.790553Z",
     "shell.execute_reply.started": "2024-09-27T21:21:30.282566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.502600</td>\n",
       "      <td>1.399642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.435100</td>\n",
       "      <td>1.299561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.291700</td>\n",
       "      <td>1.221865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.221700</td>\n",
       "      <td>1.204040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=1.3700927014936481, metrics={'train_runtime': 49.8281, 'train_samples_per_second': 54.186, 'train_steps_per_second': 3.432, 'total_flos': 3042183323910144.0, 'train_loss': 1.3700927014936481, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:29:33.793271Z",
     "iopub.status.busy": "2024-09-27T21:29:33.792969Z",
     "iopub.status.idle": "2024-09-27T21:29:35.162222Z",
     "shell.execute_reply": "2024-09-27T21:29:35.161478Z",
     "shell.execute_reply.started": "2024-09-27T21:29:33.793223Z"
    },
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:38:37.936614Z",
     "iopub.status.busy": "2024-09-27T21:38:37.935932Z",
     "iopub.status.idle": "2024-09-27T21:38:58.553883Z",
     "shell.execute_reply": "2024-09-27T21:38:58.552887Z",
     "shell.execute_reply.started": "2024-09-27T21:38:37.936581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'I bought the same item twice, cancel order {{Order Number}}', 'text': '<|im_start|>system\\n    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\\n    <|im_start|>user\\n    I bought the same item twice, cancel order {{Order Number}}<|im_end|>\\n    <|im_start|>assistant'}\n",
      "\n",
      "    I'm sorry to hear that you've bought the same item twice. I understand that you would like to cancel your order with the order number {{Order Number}}. To assist you with this, please provide me with the details of your order so that I can quickly locate and resolve the issue for you. Your satisfaction is our top priority, and I'm here to help you every step of the way.drivers\n",
      "     bordel\n"
     ]
    }
   ],
   "source": [
    "## Run inference\n",
    "\n",
    "# Tokenize input \n",
    "test_sample = {\"instruction\": \"I bought the same item twice, cancel order {{Order Number}}\"}\n",
    "test_prompt = format_chat_template_inference(test_sample)\n",
    "print(test_prompt)\n",
    "inputs = tokenize_trunc_inference(test_prompt).to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\n\\n# Llama example for this notebook\\nhttps://www.datacamp.com/tutorial/fine-tuning-llama-3-2\\nhttps://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392\\n\\n# Quantization training\\nhttps://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm\\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\\nhttps://huggingface.co/blog/hf-bitsandbytes-integration\\nhttps://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.\\n\\n# Data\\nhttps://huggingface.co/docs/transformers/main/en/chat_templating\\n\\n# Training/Lora/PEFT\\nhttps://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\\nhttps://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods\\nhttps://huggingface.co/docs/peft/main/en/developer_guides/checkpoint\\n\\n# Generation\\nhttps://huggingface.co/docs/transformers/main/en/llm_tutorial\\nhttps://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate\\n\\n# Caching & optimization\\nhttps://huggingface.co/docs/transformers/v4.47.0/en/llm_optims\\nhttps://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation\\n\\n# HF notebooks\\nhttps://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "\n",
    "# Llama example for this notebook\n",
    "https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "https://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392\n",
    "\n",
    "# Quantization training\n",
    "https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.\n",
    "\n",
    "# Data\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "# Training/Lora/PEFT\n",
    "https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods\n",
    "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint\n",
    "\n",
    "# Generation\n",
    "https://huggingface.co/docs/transformers/main/en/llm_tutorial\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate\n",
    "\n",
    "# Caching & optimization\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_optims\n",
    "https://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation\n",
    "\n",
    "# HF notebooks\n",
    "https://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

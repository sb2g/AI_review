{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to finetune LLM that has been already finetuned on instruction dataset, using hugging face trainer. \n",
    "- Llama 3.2 1B Instruct model was used as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Conda env: \n",
    "# Setup: conda env create -f environment_mlenv2\n",
    "# Activate: conda activate mlenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:06.328308Z",
     "iopub.status.busy": "2024-09-27T21:10:06.327898Z",
     "iopub.status.idle": "2024-09-27T21:10:20.481499Z",
     "shell.execute_reply": "2024-09-27T21:10:20.480549Z",
     "shell.execute_reply.started": "2024-09-27T21:10:06.328271Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch #, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:20.483420Z",
     "iopub.status.busy": "2024-09-27T21:10:20.482719Z",
     "iopub.status.idle": "2024-09-27T21:10:20.791138Z",
     "shell.execute_reply": "2024-09-27T21:10:20.790252Z",
     "shell.execute_reply.started": "2024-09-27T21:10:20.483383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)\n",
    "\n",
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:34:05.649291Z",
     "iopub.status.busy": "2024-09-27T21:34:05.648875Z",
     "iopub.status.idle": "2024-09-27T21:34:05.654251Z",
     "shell.execute_reply": "2024-09-27T21:34:05.652981Z",
     "shell.execute_reply.started": "2024-09-27T21:34:05.649255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "new_model = \"output_dir/Ecommerce-ChatBot-Instruct\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 26872\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample instruction: I need to cancel purchase {{Order Number}}\n",
      "Sample response: I understood that you need assistance with canceling your purchase with the order number {{Order Num\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Apr 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI need to cancel purchase {{Order Number}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:\\n\\n1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.\\n2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.\\n3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.\\n4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.\\n5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.\\n\\nIf you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction.<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a sample & check format\n",
    "\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "sample_instruction = dataset['instruction'][3]\n",
    "sample_response = dataset['response'][3]\n",
    "print(\"Sample instruction:\", sample_instruction[:100])\n",
    "print(\"Sample response:\", sample_response[:100])\n",
    "\n",
    "message = [ #{\"role\": \"system\", \"content\": instruction },\n",
    "            {\"role\": \"user\", \"content\": sample_instruction},\n",
    "            {\"role\": \"assistant\", \"content\": sample_response}]\n",
    "    \n",
    "tokenized_chat = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "tokenized_chat\n",
    "#print(tokenizer.decode(tokenized_chat[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Mar 2025\\n\\nYou are a top-rated customer service agent named John. \\n    Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ncould you tell me about the options for shipping?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOf course, I'd be delighted to provide you with information about our shipping options! Here are the various choices we offer:\\n\\n1. Standard Shipping: This option typically arrives within {{Date Range}} business days, catering to non-urgent items and ensuring a cost-effective delivery.\\n\\n2. Expedited Shipping: If you're looking for a faster option, choose expedited shipping. Your items will reach you within {{Date Range}} business days, offering a balance between speed and affordability.\\n\\n3. Overnight Shipping: For urgent needs, we have overnight shipping. This ensures your items are delivered on the next business day, offering the highest level of speed and convenience.\\n\\n4. In-Store Pickup: If you prefer a more hands-on approach, you can opt for in-store pickup. This option allows you to collect your items personally from one of our {{Store Location}}, offering added flexibility and convenience.\\n\\nRemember that specific delivery times may vary based on your location and other factors. If you have any further questions or need assistance with any aspect of our shipping options, please feel free to ask. We're here to ensure your satisfaction!<|eot_id|>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select subset of data for train/test & check template \n",
    "\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train & test\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:14:14.553186Z",
     "iopub.status.busy": "2024-09-27T21:14:14.552280Z",
     "iopub.status.idle": "2024-09-27T21:14:59.093678Z",
     "shell.execute_reply": "2024-09-27T21:14:59.092505Z",
     "shell.execute_reply.started": "2024-09-27T21:14:14.553130Z"
    },
    "id": "StJKGiDDHzdk",
    "outputId": "871214ba-6c30-4ecf-ac68-550f296b7ef6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "print(torch_dtype, attn_implementation)\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:15:00.330556Z",
     "iopub.status.busy": "2024-09-27T21:15:00.330315Z",
     "iopub.status.idle": "2024-09-27T21:15:01.476477Z",
     "shell.execute_reply": "2024-09-27T21:15:01.475609Z",
     "shell.execute_reply.started": "2024-09-27T21:15:00.330534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k_proj', 'o_proj', 'down_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            #print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "#tokenizer.chat_template = None # sbujimal added\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer) # sbujimal commented out\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.192045Z",
     "iopub.status.busy": "2024-09-27T21:21:29.191332Z",
     "iopub.status.idle": "2024-09-27T21:21:29.228569Z",
     "shell.execute_reply": "2024-09-27T21:21:29.227705Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.192016Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1, #2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.229887Z",
     "iopub.status.busy": "2024-09-27T21:21:29.229613Z",
     "iopub.status.idle": "2024-09-27T21:21:30.281033Z",
     "shell.execute_reply": "2024-09-27T21:21:30.280293Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.229863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90939/977504036.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer( #Trainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 900/900 [00:00<00:00, 13010.26 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 900/900 [00:00<00:00, 22535.62 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 900/900 [00:00<00:00, 2080.08 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 900/900 [00:00<00:00, 5042.22 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 100/100 [00:00<00:00, 9999.06 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 100/100 [00:00<00:00, 12400.01 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 100/100 [00:00<00:00, 1920.63 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 100/100 [00:00<00:00, 4525.33 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer( #Trainer( \n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:30.282592Z",
     "iopub.status.busy": "2024-09-27T21:21:30.282299Z",
     "iopub.status.idle": "2024-09-27T21:29:33.791645Z",
     "shell.execute_reply": "2024-09-27T21:29:33.790553Z",
     "shell.execute_reply.started": "2024-09-27T21:21:30.282566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.286700</td>\n",
       "      <td>1.309933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.087800</td>\n",
       "      <td>1.080681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.046600</td>\n",
       "      <td>1.014624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.973905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=1.243795162753055, metrics={'train_runtime': 17.7296, 'train_samples_per_second': 50.762, 'train_steps_per_second': 3.215, 'total_flos': 1131805412474880.0, 'train_loss': 1.243795162753055})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "model.config.use_cache = False\n",
    "# Pad token was not set by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:29:33.793271Z",
     "iopub.status.busy": "2024-09-27T21:29:33.792969Z",
     "iopub.status.idle": "2024-09-27T21:29:35.162222Z",
     "shell.execute_reply": "2024-09-27T21:29:35.161478Z",
     "shell.execute_reply.started": "2024-09-27T21:29:33.793223Z"
    },
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:38:37.936614Z",
     "iopub.status.busy": "2024-09-27T21:38:37.935932Z",
     "iopub.status.idle": "2024-09-27T21:38:58.553883Z",
     "shell.execute_reply": "2024-09-27T21:38:58.552887Z",
     "shell.execute_reply.started": "2024-09-27T21:38:37.936581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thank you for reaching out to us regarding your issue of buying the same item twice. We understand that you have purchased the same item {{Order Number}} twice, and we apologize for the inconvenience caused. To resolve this, we kindly request you to provide us with the necessary details, such as the order number and the date of purchase. This information will help us identify the issue and provide the appropriate solution. Your cooperation is greatly appreciated, and we are committed to ensuring that you receive the item you want. Please rest assured that we will do our best to resolve this matter promptly and efficiently. Your satisfaction is our top priority, and we appreciate your patience as we work towards a resolution. If you have any further questions or concerns, please do not hesitate\n"
     ]
    }
   ],
   "source": [
    "## Run inference\n",
    "\n",
    "# Tokenize input\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\n\\n# Llama example for this notebook\\nhttps://www.datacamp.com/tutorial/fine-tuning-llama-3-2\\nhttps://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392\\n\\n# Quantization training\\nhttps://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm\\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\\nhttps://huggingface.co/blog/hf-bitsandbytes-integration\\nhttps://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.\\n\\n# Data\\nhttps://huggingface.co/docs/transformers/main/en/chat_templating\\n\\n# Training/Lora/PEFT\\nhttps://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\\nhttps://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods\\nhttps://huggingface.co/docs/peft/main/en/developer_guides/checkpoint\\n\\n# Generation\\nhttps://huggingface.co/docs/transformers/main/en/llm_tutorial\\nhttps://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate\\n\\n# Caching & optimization\\nhttps://huggingface.co/docs/transformers/v4.47.0/en/llm_optims\\nhttps://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation\\n\\n# HF notebooks\\nhttps://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "\n",
    "# Llama example for this notebook\n",
    "https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "https://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392\n",
    "\n",
    "# Quantization training\n",
    "https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.\n",
    "\n",
    "# Data\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "# Training/Lora/PEFT\n",
    "https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods\n",
    "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint\n",
    "\n",
    "# Generation\n",
    "https://huggingface.co/docs/transformers/main/en/llm_tutorial\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate\n",
    "\n",
    "# Caching & optimization\n",
    "https://huggingface.co/docs/transformers/v4.47.0/en/llm_optims\n",
    "https://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation\n",
    "\n",
    "# HF notebooks\n",
    "https://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

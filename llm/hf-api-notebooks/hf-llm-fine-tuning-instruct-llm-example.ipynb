{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to finetune LLM that has been already finetuned on instruction dataset, using hugging face trainer. \n",
    "- Llama 3.2 1B Instruct model was used as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# Conda env: \n",
    "# Setup: conda env create -f environment_mlenv2\n",
    "# Activate: conda activate mlenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:06.328308Z",
     "iopub.status.busy": "2024-09-27T21:10:06.327898Z",
     "iopub.status.idle": "2024-09-27T21:10:20.481499Z",
     "shell.execute_reply": "2024-09-27T21:10:20.480549Z",
     "shell.execute_reply.started": "2024-09-27T21:10:06.328271Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch #, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:20.483420Z",
     "iopub.status.busy": "2024-09-27T21:10:20.482719Z",
     "iopub.status.idle": "2024-09-27T21:10:20.791138Z",
     "shell.execute_reply": "2024-09-27T21:10:20.790252Z",
     "shell.execute_reply.started": "2024-09-27T21:10:20.483383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)\n",
    "\n",
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:34:05.649291Z",
     "iopub.status.busy": "2024-09-27T21:34:05.648875Z",
     "iopub.status.idle": "2024-09-27T21:34:05.654251Z",
     "shell.execute_reply": "2024-09-27T21:34:05.652981Z",
     "shell.execute_reply.started": "2024-09-27T21:34:05.649255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "new_model = \"output_dir/Ecommerce-ChatBot-Instruct\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 26872\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample instruction: I need to cancel purchase {{Order Number}}\n",
      "Sample response: I understood that you need assistance with canceling your purchase with the order number {{Order Num\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Apr 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI need to cancel purchase {{Order Number}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:\\n\\n1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.\\n2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.\\n3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.\\n4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.\\n5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.\\n\\nIf you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction.<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a sample & check format\n",
    "\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "sample_instruction = dataset['instruction'][3]\n",
    "sample_response = dataset['response'][3]\n",
    "print(\"Sample instruction:\", sample_instruction[:100])\n",
    "print(\"Sample response:\", sample_response[:100])\n",
    "\n",
    "message = [ #{\"role\": \"system\", \"content\": instruction },\n",
    "            {\"role\": \"user\", \"content\": sample_instruction},\n",
    "            {\"role\": \"assistant\", \"content\": sample_response}]\n",
    "    \n",
    "tokenized_chat = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "tokenized_chat\n",
    "#print(tokenizer.decode(tokenized_chat[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:00<00:00, 10900.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Apr 2025\\n\\nYou are a top-rated customer service agent named John. \\n    Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI need to cancel purchase {{Order Number}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:\\n\\n1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.\\n2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.\\n3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.\\n4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.\\n5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.\\n\\nIf you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction.<|eot_id|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select subset of data for train/test & check template \n",
    "\n",
    "#dataset = dataset.shuffle(seed=65).select(range(5000))\n",
    "dataset = dataset.select(range(5000))\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 4500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train & test\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:14:14.553186Z",
     "iopub.status.busy": "2024-09-27T21:14:14.552280Z",
     "iopub.status.idle": "2024-09-27T21:14:59.093678Z",
     "shell.execute_reply": "2024-09-27T21:14:59.092505Z",
     "shell.execute_reply.started": "2024-09-27T21:14:14.553130Z"
    },
    "id": "StJKGiDDHzdk",
    "outputId": "871214ba-6c30-4ecf-ac68-550f296b7ef6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "print(torch_dtype, attn_implementation)\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:15:00.330556Z",
     "iopub.status.busy": "2024-09-27T21:15:00.330315Z",
     "iopub.status.idle": "2024-09-27T21:15:01.476477Z",
     "shell.execute_reply": "2024-09-27T21:15:01.475609Z",
     "shell.execute_reply.started": "2024-09-27T21:15:00.330534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'k_proj', 'down_proj', 'v_proj', 'up_proj', 'q_proj', 'gate_proj']\n"
     ]
    }
   ],
   "source": [
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            #print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "#tokenizer.chat_template = None # sbujimal added\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer) # sbujimal commented out\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.192045Z",
     "iopub.status.busy": "2024-09-27T21:21:29.191332Z",
     "iopub.status.idle": "2024-09-27T21:21:29.228569Z",
     "shell.execute_reply": "2024-09-27T21:21:29.227705Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.192016Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1, #2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.229887Z",
     "iopub.status.busy": "2024-09-27T21:21:29.229613Z",
     "iopub.status.idle": "2024-09-27T21:21:30.281033Z",
     "shell.execute_reply": "2024-09-27T21:21:30.280293Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.229863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181917/977504036.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer( #Trainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 4500/4500 [00:00<00:00, 15229.46 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 4500/4500 [00:00<00:00, 24294.90 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 4500/4500 [00:02<00:00, 2107.00 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 4500/4500 [00:00<00:00, 5497.38 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 500/500 [00:00<00:00, 14404.31 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 500/500 [00:00<00:00, 21014.81 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 500/500 [00:00<00:00, 2240.46 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 500/500 [00:00<00:00, 5448.59 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer( #Trainer( \n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:30.282592Z",
     "iopub.status.busy": "2024-09-27T21:21:30.282299Z",
     "iopub.status.idle": "2024-09-27T21:29:33.791645Z",
     "shell.execute_reply": "2024-09-27T21:29:33.790553Z",
     "shell.execute_reply.started": "2024-09-27T21:21:30.282566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 01:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.667337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.564961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.521300</td>\n",
       "      <td>0.506749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>0.482343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=0.6259131219158781, metrics={'train_runtime': 69.4425, 'train_samples_per_second': 64.802, 'train_steps_per_second': 4.061, 'total_flos': 4831501409992704.0, 'train_loss': 0.6259131219158781})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "model.config.use_cache = False\n",
    "# Pad token was not set by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:29:33.793271Z",
     "iopub.status.busy": "2024-09-27T21:29:33.792969Z",
     "iopub.status.idle": "2024-09-27T21:29:35.162222Z",
     "shell.execute_reply": "2024-09-27T21:29:35.161478Z",
     "shell.execute_reply.started": "2024-09-27T21:29:33.793223Z"
    },
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:38:37.936614Z",
     "iopub.status.busy": "2024-09-27T21:38:37.935932Z",
     "iopub.status.idle": "2024-09-27T21:38:58.553883Z",
     "shell.execute_reply": "2024-09-27T21:38:58.552887Z",
     "shell.execute_reply.started": "2024-09-27T21:38:37.936581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"max_new_tokens\": 150,\n",
      "  \"pad_token_id\": 128009,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"temperature\": 0.05\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "I'm sorry to hear that you're experiencing difficulties with canceling your order. To assist you further, could you please provide me with the details of the item you would like to cancel? This will help me ensure that we address your concerns accurately and promptly. Your satisfaction is our top priority, and we're here to make things right for you. Thank you for reaching out to us. We appreciate your patience and cooperation in resolving this matter. Your order number is {{Order Number}}. We understand that you would like to cancel your purchase of the item. To cancel your order, please follow these steps:\n",
      "\n",
      "1. Log in to your account on our website.\n",
      "2. Navigate to the \"My Orders\" section.\n",
      "3. Locate the specific order\n"
     ]
    }
   ],
   "source": [
    "## Run inference\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig(\n",
    "    #max_length=256,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.05,\n",
    "    do_sample=True,\n",
    "    #do_sample=False,\n",
    "    use_cache=True,\n",
    "    skip_special_tokens=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(generation_config)\n",
    "\n",
    "# Test input\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "# Tokenize input\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "#outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> Quantization training\n",
    ">> https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm  \n",
    ">> https://huggingface.co/blog/4bit-transformers-bitsandbytes  \n",
    ">> https://huggingface.co/blog/hf-bitsandbytes-integration  \n",
    ">> https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers \n",
    "\n",
    "> Data\n",
    ">> https://huggingface.co/docs/transformers/main/en/chat_templating \n",
    "\n",
    "> Training/Lora/PEFT\n",
    ">> https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments  \n",
    ">> https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods  \n",
    ">> https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint  \n",
    "\n",
    "> Generation\n",
    ">> https://huggingface.co/docs/transformers/main/en/llm_tutorial  \n",
    ">> https://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate  \n",
    "\n",
    "> Llama example \n",
    ">> https://www.datacamp.com/tutorial/fine-tuning-llama-3-2  \n",
    ">> https://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-2-on-customer-support/notebook?scriptVersionId=198573392  \n",
    "\n",
    "> Caching & optimization\n",
    ">> https://huggingface.co/docs/transformers/v4.47.0/en/llm_optims  \n",
    ">> https://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation  \n",
    "\n",
    "> HF notebooks\n",
    ">> https://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

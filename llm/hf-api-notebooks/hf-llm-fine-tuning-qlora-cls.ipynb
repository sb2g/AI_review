{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to finetune language model using QLoRA -- using BitsandBytes library & hugging face trainer. \n",
    "- \"distill-bert model\" was used as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QLoRA\n",
    "\n",
    "- QLoRA is an extension of LoRA that leverages quantization\n",
    "- It reduces memory footprint by using fewer number of bits to store information\n",
    "- Smaller models => Less resources for training & inference\n",
    "- Compared to LoRA: Accuracy is usually comparable, Fine tuning speed may increase/decrease depending on hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Conda env: \n",
    "# Setup: conda env create -f environment_mlenv2\n",
    "# Activate: conda activate mlenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    TaskType, \n",
    "    replace_lora_weights_loftq,\n",
    ")\n",
    "import torch #, wandb\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)\n",
    "\n",
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"distilbert-base-uncased\"\n",
    "new_model = \"../output_dir/imdb-cls\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Load slow tokenizer, fast tokenizer sometimes ignores added tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: This film was probably inspired by Godard's Masculin, f√©minin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "# Check a sample & check format\n",
    "sample_text = dataset['train']['text'][3]\n",
    "sample_label = dataset['train']['label'][3]\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Sample label:\", sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 unique labels in the dataset:\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "# Get all unique labels\n",
    "train_labels = dataset['train']['label']\n",
    "unique_labels = set(train_labels)\n",
    "print(f\"Found {len(unique_labels)} unique labels in the dataset:\")\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Check tokenize\n",
    "tokenized_sample = tokenizer(sample_text)\n",
    "print(tokenized_sample)\n",
    "\n",
    "print(tokenized_sample.keys())\n",
    "# If token_type_ids is present, print it\n",
    "if 'token_type_ids' in tokenized_sample:\n",
    "    print(\"Token Type IDs:\", tokenized_sample['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproces function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train tokenized:\n",
      " {'input_ids': [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check sample prompt templates\n",
    "sample_text = dataset['train']['text'][3]\n",
    "sample_label = dataset['train']['label'][3]\n",
    "train_sample = {\"text\": sample_text, \n",
    "                \"label\": sample_label}\n",
    "sample_text_tokenized = preprocess_function(train_sample)\n",
    "print(\"Sample train tokenized:\\n\", sample_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Check a train sample:\n",
      "{'text': \"I was pleasantly surprised by the film. Let's face it; the premise doesn't sound particularly appealing when having to hand over money for a the night's flick, but it had an easygoing nature that wins one over. There were no moments that I found uproarious, and I doubt any that I'll remember the next day, but this doesn't fail as a nice diversion. What I found funny was watching it here in Peking with my Chinese girlfriend who never understands anything I like. I told her there was a plot- three guys have to bring back some weed to London. Hardly satisfying for her. There is no mugging going on here for the camera which I'd been expecting after reading a number of the comments. I do take exception however to comparisons with Withnail and I; not in the same league, and I doubt was it intended to. www.imperialflags.blogspot.com\", 'label': 1, 'input_ids': [101, 1045, 2001, 27726, 4527, 2011, 1996, 2143, 1012, 2292, 1005, 1055, 2227, 2009, 1025, 1996, 18458, 2987, 1005, 1056, 2614, 3391, 16004, 2043, 2383, 2000, 2192, 2058, 2769, 2005, 1037, 1996, 2305, 1005, 1055, 17312, 1010, 2021, 2009, 2018, 2019, 3733, 26966, 3267, 2008, 5222, 2028, 2058, 1012, 2045, 2020, 2053, 5312, 2008, 1045, 2179, 2039, 3217, 16843, 2271, 1010, 1998, 1045, 4797, 2151, 2008, 1045, 1005, 2222, 3342, 1996, 2279, 2154, 1010, 2021, 2023, 2987, 1005, 1056, 8246, 2004, 1037, 3835, 20150, 1012, 2054, 1045, 2179, 6057, 2001, 3666, 2009, 2182, 1999, 27057, 2007, 2026, 2822, 6513, 2040, 2196, 19821, 2505, 1045, 2066, 1012, 1045, 2409, 2014, 2045, 2001, 1037, 5436, 1011, 2093, 4364, 2031, 2000, 3288, 2067, 2070, 17901, 2000, 2414, 1012, 6684, 17087, 2005, 2014, 1012, 2045, 2003, 2053, 14757, 4726, 2183, 2006, 2182, 2005, 1996, 4950, 2029, 1045, 1005, 1040, 2042, 8074, 2044, 3752, 1037, 2193, 1997, 1996, 7928, 1012, 1045, 2079, 2202, 6453, 2174, 2000, 18539, 2007, 2007, 25464, 1998, 1045, 1025, 2025, 1999, 1996, 2168, 2223, 1010, 1998, 1045, 4797, 2001, 2009, 3832, 2000, 1012, 7479, 1012, 4461, 10258, 26454, 1012, 23012, 11008, 1012, 4012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "1\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Check a test sample:\n",
      "{'text': 'Hey there Army Sgt. I\\'m sorry dude but being a SGT in the Army and being in the Army National Guard does not make you qualified to comment on a Marine movie. You are not a Marine and just because you wear a uniform doesn\\'t mean you can relate to being a Marine. We simply are the best, we have the hardest training, yes we have big heads about ourselves, but hey when you are the best, you like to strut your stuff. I was in the Iraq invasion and in Fallujah. I fought next to soldiers. You are not \"qualified\" to say anything about my Marine Corps. I hate to be the one that starts the whole \"which branch is better\", but you have no right to say you are qualified to judge a Marine movie. Oh yeah......we are Drill Instructors.......not Drill SGT\\'s. That\\'s the biggest clue you have no idea about what you are talking about. Yeah we do not \"curse\" at recruits anymore. Tell me, how is cussing at someone going to make them a better Marine? How will me hitting someone make a Marine a better Marine? Yes it is a kinder boot camp from what I went through. But we are dealing with different times and people. We are training people who are over all smarter than our generations recruits. We want smarter recruits, not meaner. And anyone who signs up to be a Marine in the first place, has a dedication to be the best his country has to offer. We don\\'t have to reinforce that in Bootcamp. Marines come to Bootcamp wanting to be killers. We don\\'t need to teach them that by demoralizing them by swearing at them and beating them. At least that is how I feel.And yes, I am \"qualified\" to say that. I have been on the battlefield numerous times and I have trained Marines and Recruits who eventually ended up on the battlefield. But then again, what do I know. I was just there, done that, got the t-shirt. SGT of the Army.......get a clue!', 'label': 1, 'input_ids': [101, 4931, 2045, 2390, 17001, 1012, 1045, 1005, 1049, 3374, 12043, 2021, 2108, 1037, 17001, 1999, 1996, 2390, 1998, 2108, 1999, 1996, 2390, 2120, 3457, 2515, 2025, 2191, 2017, 4591, 2000, 7615, 2006, 1037, 3884, 3185, 1012, 2017, 2024, 2025, 1037, 3884, 1998, 2074, 2138, 2017, 4929, 1037, 6375, 2987, 1005, 1056, 2812, 2017, 2064, 14396, 2000, 2108, 1037, 3884, 1012, 2057, 3432, 2024, 1996, 2190, 1010, 2057, 2031, 1996, 18263, 2731, 1010, 2748, 2057, 2031, 2502, 4641, 2055, 9731, 1010, 2021, 4931, 2043, 2017, 2024, 1996, 2190, 1010, 2017, 2066, 2000, 2358, 22134, 2115, 4933, 1012, 1045, 2001, 1999, 1996, 5712, 5274, 1998, 1999, 2991, 23049, 4430, 1012, 1045, 4061, 2279, 2000, 3548, 1012, 2017, 2024, 2025, 1000, 4591, 1000, 2000, 2360, 2505, 2055, 2026, 3884, 3650, 1012, 1045, 5223, 2000, 2022, 1996, 2028, 2008, 4627, 1996, 2878, 1000, 2029, 3589, 2003, 2488, 1000, 1010, 2021, 2017, 2031, 2053, 2157, 2000, 2360, 2017, 2024, 4591, 2000, 3648, 1037, 3884, 3185, 1012, 2821, 3398, 1012, 1012, 1012, 1012, 1012, 1012, 2057, 2024, 12913, 19922, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 2025, 12913, 17001, 1005, 1055, 1012, 2008, 1005, 1055, 1996, 5221, 9789, 2017, 2031, 2053, 2801, 2055, 2054, 2017, 2024, 3331, 2055, 1012, 3398, 2057, 2079, 2025, 1000, 8364, 1000, 2012, 15024, 4902, 1012, 2425, 2033, 1010, 2129, 2003, 12731, 18965, 2012, 2619, 2183, 2000, 2191, 2068, 1037, 2488, 3884, 1029, 2129, 2097, 2033, 7294, 2619, 2191, 1037, 3884, 1037, 2488, 3884, 1029, 2748, 2009, 2003, 1037, 2785, 2121, 9573, 3409, 2013, 2054, 1045, 2253, 2083, 1012, 2021, 2057, 2024, 7149, 2007, 2367, 2335, 1998, 2111, 1012, 2057, 2024, 2731, 2111, 2040, 2024, 2058, 2035, 25670, 2084, 2256, 8213, 15024, 1012, 2057, 2215, 25670, 15024, 1010, 2025, 2812, 2121, 1012, 1998, 3087, 2040, 5751, 2039, 2000, 2022, 1037, 3884, 1999, 1996, 2034, 2173, 1010, 2038, 1037, 12276, 2000, 2022, 1996, 2190, 2010, 2406, 2038, 2000, 3749, 1012, 2057, 2123, 1005, 1056, 2031, 2000, 19444, 2008, 1999, 9573, 26468, 1012, 9622, 2272, 2000, 9573, 26468, 5782, 2000, 2022, 15978, 1012, 2057, 2123, 1005, 1056, 2342, 2000, 6570, 2068, 2008, 2011, 9703, 7941, 6026, 2068, 2011, 25082, 2012, 2068, 1998, 6012, 2068, 1012, 2012, 2560, 2008, 2003, 2129, 1045, 2514, 1012, 1998, 2748, 1010, 1045, 2572, 1000, 4591, 1000, 2000, 2360, 2008, 1012, 1045, 2031, 2042, 2006, 1996, 11686, 3365, 2335, 1998, 1045, 2031, 4738, 9622, 1998, 15024, 2040, 2776, 3092, 2039, 2006, 1996, 11686, 1012, 2021, 2059, 2153, 1010, 2054, 2079, 1045, 2113, 1012, 1045, 2001, 2074, 2045, 1010, 2589, 2008, 1010, 2288, 1996, 1056, 1011, 3797, 1012, 17001, 1997, 1996, 2390, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 2131, 1037, 9789, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Select subset of data for train/test & format prompt template \n",
    "def data_prep(dataset, subset_type):\n",
    "    dataset_subset = dataset[subset_type].shuffle(seed=65).select(range(5000))\n",
    "    #dataset_subset = dataset[subset_type].select(range(5000))\n",
    "    dataset_subset = dataset_subset.map(preprocess_function, num_proc= 4, batched=True)\n",
    "    print(dataset_subset)\n",
    "    print(f\"Check a {subset_type} sample:\")\n",
    "    print(dataset_subset[3])\n",
    "    print(dataset_subset['label'][3])\n",
    "    return dataset_subset\n",
    "\n",
    "train_data = data_prep(dataset, 'train')\n",
    "test_data = data_prep(dataset, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "print(torch_dtype, attn_implementation)\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                  # quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\",          # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_compute_dtype=torch_dtype, # use bfloat16 for faster computation\n",
    "    bnb_4bit_use_double_quant=True,     # nested quantization scheme to quantize the already quantized weights\n",
    "    llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"] #  Don't convert the \"classifier\" and \"pre_classifier\" layers to 8-bit\n",
    ")\n",
    "\n",
    "# Give labels to classes\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = dict((v,k) for k,v in id2label.items())\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=2,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    #attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Linear layers are converted to Linear4bit layers. These use blockwise k-bit quantization under the hood\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['out_lin', 'q_lin', 'v_lin', 'lin1', 'k_lin', 'lin2']\n"
     ]
    }
   ],
   "source": [
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            #print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                                        # Rank of the low-rank matrices\n",
    "    lora_alpha=16,                              # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,                  # instead of \"CAUSAL_LM\",\n",
    "    target_modules=['q_lin','k_lin','v_lin']   # could be all \"modules\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the LoRA weights using LoftQ\n",
    "replace_lora_weights_loftq(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_lin): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 813,314 || all params: 67,768,324 || trainable%: 1.2001\n"
     ]
    }
   ],
   "source": [
    "# Only 1% of parameters will be trained\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    #gradient_accumulation_steps=1, #2,\n",
    "    #optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    #eval_steps=0.2,\n",
    "    #logging_steps=1,\n",
    "    #warmup_steps=10,\n",
    "    #logging_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    #fp16=False,\n",
    "    #bf16=False,\n",
    "    #group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    #report_to=\"tensorboard\",\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\", trust_remote_code=True)\n",
    " \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "   return {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = Trainer( \n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=collate,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2504' max='2504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2504/2504 04:44, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.389577</td>\n",
       "      <td>0.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.307561</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.297783</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.292212</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.288328</td>\n",
       "      <td>0.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.286440</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.286997</td>\n",
       "      <td>0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.284190</td>\n",
       "      <td>0.885400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2504, training_loss=0.3258430707854585, metrics={'train_runtime': 284.7791, 'train_samples_per_second': 140.46, 'train_steps_per_second': 8.793, 'total_flos': 5398635970560000.0, 'train_loss': 0.3258430707854585, 'epoch': 8.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "# model.config.use_cache = False\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "#model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#trainer.model.save_pretrained(new_model)\n",
    "trainer.save_model(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f46b68de580>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot eval accuracy & loss\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "get_metric = lambda metric, log_history: [log[metric] for log in log_history if metric in log]\n",
    "\n",
    "eval_accuracy = get_metric('eval_accuracy',log_history)\n",
    "eval_loss = get_metric('eval_loss',log_history)\n",
    "\n",
    "plt.plot(eval_accuracy,label='eval_accuracy')\n",
    "plt.plot(eval_loss,label='eval_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Run inference\n",
    "\n",
    "# TBD\n",
    "\n",
    "# # Test input\n",
    "# test_sample = \n",
    "\n",
    "# # Tokenize input \n",
    "# inputs = tokenize(test_sample).to(\"cuda\")\n",
    "\n",
    "# # Generate model output\n",
    "# #outputs = model(**inputs)\n",
    "\n",
    "# # Decode\n",
    "# text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> https://www.coursera.org/specializations/generative-ai-engineering-with-llms  \n",
    "> LoftQ: https://arxiv.org/abs/2310.08659\n",
    "\n",
    "> https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm  \n",
    "> https://huggingface.co/blog/4bit-transformers-bitsandbytes  \n",
    "> https://huggingface.co/blog/hf-bitsandbytes-integration  \n",
    "> https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers \n",
    "\n",
    "> https://huggingface.co/docs/transformers/main/en/chat_templating \n",
    "\n",
    "> https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments  \n",
    "> https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods  \n",
    "> https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

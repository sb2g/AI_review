{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to finetune LLM that has not be finetuned yet on instruction dataset using hugging face trainer. \n",
    "- So there will be new tokens added into the tokenizer before finetuning. \n",
    "- Llama 3.2 1B model was used as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Conda env: \n",
    "# Setup: conda env create -f environment_mlenv2\n",
    "# Activate: conda activate mlenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:10:06.328308Z",
     "iopub.status.busy": "2024-09-27T21:10:06.327898Z",
     "iopub.status.idle": "2024-09-27T21:10:20.481499Z",
     "shell.execute_reply": "2024-09-27T21:10:20.480549Z",
     "shell.execute_reply.started": "2024-09-27T21:10:06.328271Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch #, wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token = hf_token)\n",
    "\n",
    "# wb_token = user_secrets.get_secret(\"wandb\")\n",
    "# wandb.login(key=wb_token)\n",
    "# run = wandb.init(\n",
    "#     project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:34:05.649291Z",
     "iopub.status.busy": "2024-09-27T21:34:05.648875Z",
     "iopub.status.idle": "2024-09-27T21:34:05.654251Z",
     "shell.execute_reply": "2024-09-27T21:34:05.652981Z",
     "shell.execute_reply.started": "2024-09-27T21:34:05.649255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "base_model = \"meta-llama/Llama-3.2-1B\"\n",
    "new_model = \"../output_dir/Ecommerce-ChatBot\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Load slow tokenizer, fast tokenizer sometimes ignores added tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 26872\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample instruction: I need to cancel purchase {{Order Number}}\n",
      "Sample response: I understood that you need assistance with canceling your purchase with the order number {{Order Num\n"
     ]
    }
   ],
   "source": [
    "# Check a sample & check format\n",
    "sample_instruction = dataset['instruction'][3]\n",
    "sample_response = dataset['response'][3]\n",
    "print(\"Sample instruction:\", sample_instruction[:100])\n",
    "print(\"Sample response:\", sample_response[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 40, 1205, 311, 9299, 7782, 5991, 4531, 5742, 3500], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check tokenize\n",
    "tokenized_sample = tokenizer(sample_instruction)\n",
    "print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat template for training\n",
    "def format_chat_template(row) -> str:\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"\"\"\n",
    "    prompt=f\"\"\"<|im_start|>system\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {row[\"instruction\"]}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {row[\"response\"]}<|im_end|>\"\"\"\n",
    "\n",
    "    row['text'] = prompt\n",
    "    return row\n",
    "\n",
    "# Chat template for inference\n",
    "def format_chat_template_inference(row) -> str:\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"\"\"\n",
    "    prompt=f\"\"\"<|im_start|>system\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {row[\"instruction\"]}<|im_end|>\n",
    "    <|im_start|>assistant\"\"\"\n",
    "\n",
    "    row['text'] = prompt\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train prompt:\n",
      " <|im_start|>system\n",
      "    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\n",
      "    <|im_start|>user\n",
      "    I bought the same item twice, cancel order {{Order Number}}<|im_end|>\n",
      "    <|im_start|>assistant\n",
      "    Sure, can you give me a minute<|im_end|>\n",
      "Sample test prompt:\n",
      " <|im_start|>system\n",
      "    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\n",
      "    <|im_start|>user\n",
      "    I bought the same item twice, cancel order {{Order Number}}<|im_end|>\n",
      "    <|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "# Check sample prompt templates\n",
    "train_sample = {\"instruction\": \"I bought the same item twice, cancel order {{Order Number}}\", \n",
    "                \"response\": \"Sure, can you give me a minute\"}\n",
    "train_prompt = format_chat_template(train_sample)\n",
    "print(\"Sample train prompt:\\n\", train_prompt['text'])\n",
    "\n",
    "test_sample = {\"instruction\": \"I bought the same item twice, cancel order {{Order Number}}\"}\n",
    "test_prompt = format_chat_template_inference(test_sample)\n",
    "print(\"Sample test prompt:\\n\", test_prompt['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 27, 91, 318, 5011, 91, 29, 9125, 198, 262, 1472, 527, 264, 1948, 55985, 6130, 2532, 8479, 7086, 3842, 13, 2893, 48887, 311, 6444, 323, 4320, 682, 872, 4860, 16134, 91, 318, 6345, 91, 397, 262, 83739, 318, 5011, 91, 29, 882, 198, 262, 358, 11021, 279, 1890, 1537, 11157, 11, 9299, 2015, 5991, 4531, 5742, 3500, 27, 91, 318, 6345, 91, 397, 262, 83739, 318, 5011, 91, 29, 78191, 198, 262, 23371, 11, 649, 499, 3041, 757, 264, 9568, 27, 91, 318, 6345, 91, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check tokenize\n",
    "tokenized_sample = tokenizer(train_prompt['text'])\n",
    "print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:00<00:00, 17774.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Check a train sample:\n",
      "<|im_start|>system\n",
      "    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\n",
      "    <|im_start|>user\n",
      "    I need to cancel purchase {{Order Number}}<|im_end|>\n",
      "    <|im_start|>assistant\n",
      "    I understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:\n",
      "\n",
      "1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.\n",
      "2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.\n",
      "3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.\n",
      "4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.\n",
      "5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.\n",
      "\n",
      "If you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction.<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Select subset of data for train/test & format prompt template \n",
    "if 1:\n",
    "    #dataset = dataset.shuffle(seed=65).select(range(5000))\n",
    "    dataset = dataset.select(range(5000))\n",
    "    dataset = dataset.map(format_chat_template, num_proc= 4)\n",
    "    print(dataset)\n",
    "    print(\"Check a train sample:\")\n",
    "    print(dataset['text'][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:00<00:00, 5110.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:00<00:00, 6198.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize without truncation\n",
    "def tokenize(element):\n",
    "    return tokenizer(element['text'])\n",
    "\n",
    "# Tokenize with truncation\n",
    "def tokenize_trunc(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=int(1.5*max_length),\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "# Tokenize with truncation for inference\n",
    "def tokenize_trunc_inference(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=int(1.5*max_length),\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "# Add special tokens if needed\n",
    "if 1:\n",
    "    # Add tokens <|im_start|> and <|im_end|>, latter is special eos token\n",
    "    tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "    tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "    #tokenizer.add_special_tokens(dict(pad_token=\"</s>\"))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Apply tokenization - get max length\n",
    "if 1:\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize, \n",
    "        num_proc=4\n",
    "    )\n",
    "    max_length = max([len(x['input_ids']) for x in tokenized_dataset])\n",
    "    print(max_length)\n",
    "\n",
    "# Apply tokenization - Tokenize with max length\n",
    "if 1:\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_trunc,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=[\"text\"]    \n",
    "    )\n",
    "    print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['flags', 'instruction', 'category', 'intent', 'response', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train & test\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "# Transforms list of dicts [ {input_ids: [123, ..]}, {.. ] \n",
    "# into dict of lists (pytorch tensors) { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "# Label shifting should be handled inside the HF model forward function, so they dont need to be shifted here & can be kept same as inputs\n",
    "def collate(tokenized_batch_data):\n",
    "\n",
    "    tokenlist = [e[\"input_ids\"] for e in tokenized_batch_data]\n",
    "    tokens_maxlen = max([len(t) for t in tokenlist])  # length of longest input\n",
    "\n",
    "    input_ids, labels, attention_masks = [], [], []\n",
    "    for tokens in tokenlist:\n",
    "        # Num of pad tokens to add\n",
    "        pad_len = tokens_maxlen-len(tokens)\n",
    "        # Pad\n",
    "        input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )\n",
    "        labels.append( tokens + [-100]*pad_len )\n",
    "        attention_masks.append( [1]*len(tokens) + [0]*pad_len )\n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "        \"attention_mask\": torch.tensor(attention_masks)\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "print(torch_dtype, attn_implementation)\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Update model config based on tokenizer update\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'up_proj', 'q_proj', 'o_proj', 'k_proj', 'gate_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            #print(name)\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:15:00.330556Z",
     "iopub.status.busy": "2024-09-27T21:15:00.330315Z",
     "iopub.status.idle": "2024-09-27T21:15:01.476477Z",
     "shell.execute_reply": "2024-09-27T21:15:01.475609Z",
     "shell.execute_reply.started": "2024-09-27T21:15:00.330534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.192045Z",
     "iopub.status.busy": "2024-09-27T21:21:29.191332Z",
     "iopub.status.idle": "2024-09-27T21:21:29.228569Z",
     "shell.execute_reply": "2024-09-27T21:21:29.227705Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.192016Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1, #2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    #report_to=\"wandb\"\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:29.229887Z",
     "iopub.status.busy": "2024-09-27T21:21:29.229613Z",
     "iopub.status.idle": "2024-09-27T21:21:30.281033Z",
     "shell.execute_reply": "2024-09-27T21:21:30.280293Z",
     "shell.execute_reply.started": "2024-09-27T21:21:29.229863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = Trainer( \n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    #peft_config=peft_config,\n",
    "    #max_seq_length= 512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    #tokenizer=tokenizer,\n",
    "    data_collator=collate,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:21:30.282592Z",
     "iopub.status.busy": "2024-09-27T21:21:30.282299Z",
     "iopub.status.idle": "2024-09-27T21:29:33.791645Z",
     "shell.execute_reply": "2024-09-27T21:29:33.790553Z",
     "shell.execute_reply.started": "2024-09-27T21:21:30.282566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>1.352123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.223600</td>\n",
       "      <td>1.243121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.131600</td>\n",
       "      <td>1.183859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.122600</td>\n",
       "      <td>1.133035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=1.2798068168315482, metrics={'train_runtime': 69.6842, 'train_samples_per_second': 64.577, 'train_steps_per_second': 4.047, 'total_flos': 4298474337435648.0, 'train_loss': 1.2798068168315482, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching k, v. Its on by default in model config. Not useful for training, only needed for generation\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:29:33.793271Z",
     "iopub.status.busy": "2024-09-27T21:29:33.792969Z",
     "iopub.status.idle": "2024-09-27T21:29:35.162222Z",
     "shell.execute_reply": "2024-09-27T21:29:35.161478Z",
     "shell.execute_reply.started": "2024-09-27T21:29:33.793223Z"
    },
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#wandb.finish()\n",
    "\n",
    "# Enable caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T21:38:37.936614Z",
     "iopub.status.busy": "2024-09-27T21:38:37.935932Z",
     "iopub.status.idle": "2024-09-27T21:38:58.553883Z",
     "shell.execute_reply": "2024-09-27T21:38:58.552887Z",
     "shell.execute_reply.started": "2024-09-27T21:38:37.936581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128257,\n",
      "  \"max_new_tokens\": 250,\n",
      "  \"pad_token_id\": 128257,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"temperature\": 0.05\n",
      "}\n",
      "\n",
      "{'instruction': 'I bought the same item twice, cancel order {{Order Number}}', 'text': '<|im_start|>system\\n    You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.<|im_end|>\\n    <|im_start|>user\\n    I bought the same item twice, cancel order {{Order Number}}<|im_end|>\\n    <|im_start|>assistant'}\n",
      "\n",
      "    We understand that you have purchased the same item twice. We apologize for any inconvenience caused. To cancel your order with the order number {{Order Number}}, please follow these steps:\n",
      "\n",
      "1. Log in to your account on our website.\n",
      "2. Navigate to the \"My Orders\" section.\n",
      "3. Locate the order with the order number {{Order Number}}.\n",
      "4. Click on the order to view the details.\n",
      "5. Look for the option to \"Cancel Order\" and select it.\n",
      "6. Follow any additional prompts or instructions to complete the cancellation process.\n",
      "\n",
      "If you encounter any difficulties or have any further questions, please don't hesitate to reach out to our customer support team. We are here to assist you every step of the way. Yourself Yourselfी\n",
      "     PSupport\n"
     ]
    }
   ],
   "source": [
    "## Run inference\n",
    "\n",
    "# Generation config\n",
    "generation_config = GenerationConfig(\n",
    "    #max_length=256,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.05,\n",
    "    do_sample=True,\n",
    "    #do_sample=False,\n",
    "    use_cache=True,\n",
    "    skip_special_tokens=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(generation_config)\n",
    "\n",
    "# Test input\n",
    "test_sample = {\"instruction\": \"I bought the same item twice, cancel order {{Order Number}}\"}\n",
    "\n",
    "# Tokenize input \n",
    "test_prompt = format_chat_template_inference(test_sample)\n",
    "print(test_prompt)\n",
    "inputs = tokenize_trunc_inference(test_prompt).to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "#outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> Quantization training\n",
    ">> https://huggingface.co/docs/transformers/en/quantization/bitsandbytes#4-bit-qlora-algorithm  \n",
    ">> https://huggingface.co/blog/4bit-transformers-bitsandbytes  \n",
    ">> https://huggingface.co/blog/hf-bitsandbytes-integration  \n",
    ">> https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers \n",
    "\n",
    "> Data\n",
    ">> https://huggingface.co/docs/transformers/main/en/chat_templating \n",
    "\n",
    "> Training/Lora/PEFT\n",
    ">> https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments  \n",
    ">> https://huggingface.co/docs/peft/v0.14.0/en/task_guides/lora_based_methods  \n",
    ">> https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint  \n",
    "\n",
    "> Generation\n",
    ">> https://huggingface.co/docs/transformers/main/en/llm_tutorial  \n",
    ">> https://huggingface.co/docs/transformers/v4.47.0/en/llm_tutorial#default-generate  \n",
    "\n",
    "> Caching & optimization\n",
    ">> https://huggingface.co/docs/transformers/v4.47.0/en/llm_optims  \n",
    ">> https://huggingface.co/docs/transformers/en/kv_cache#re-use-cache-to-continue-generation  \n",
    "\n",
    "> HF notebooks\n",
    ">> https://github.com/huggingface/notebooks/tree/main/transformers_doc/en/pytorch  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

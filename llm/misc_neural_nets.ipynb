{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook has some miscellaneous items related to Neural nets:\n",
    "    - Embeddings \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "- Also implement it inefficiently using FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x783e7c16f390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparams\n",
    "vocab_size = 5\n",
    "emb_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) Parameter containing:\n",
      "tensor([[-0.0404,  1.7260, -0.8140],\n",
      "        [ 1.3722,  0.5060, -0.4823],\n",
      "        [-0.7853,  0.6681, -0.4439],\n",
      "        [ 0.1888,  0.5986,  0.6458],\n",
      "        [ 0.6306, -1.4668, -0.6798]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define embedding layer\n",
    "emb_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
    "print(emb_layer.weight.shape, emb_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) tensor([2, 1])\n",
      "torch.Size([2, 3]) tensor([[-0.7853,  0.6681, -0.4439],\n",
      "        [ 1.3722,  0.5060, -0.4823]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Sample embedding for tokens 2, 1\n",
    "sample_token_ids = torch.tensor([2, 1])\n",
    "sample_emb = emb_layer(sample_token_ids) # --> 3rd, 2nd rows of embedding layer\n",
    "print(sample_token_ids.shape, sample_token_ids)\n",
    "print(sample_emb.shape, sample_emb)\n",
    "print(sample_emb == emb_layer.weight[sample_token_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) Parameter containing:\n",
      "tensor([[ 0.4318,  0.1762,  0.1705, -0.0531,  0.0578],\n",
      "        [-0.2760, -0.1286,  0.0560, -0.4053, -0.0430],\n",
      "        [-0.0369,  0.2302,  0.4409,  0.0130, -0.0426]], requires_grad=True)\n",
      "torch.Size([3, 5]) Parameter containing:\n",
      "tensor([[-0.0404,  1.3722, -0.7853,  0.1888,  0.6306],\n",
      "        [ 1.7260,  0.5060,  0.6681,  0.5986, -1.4668],\n",
      "        [-0.8140, -0.4823, -0.4439,  0.6458, -0.6798]], requires_grad=True)\n",
      "torch.Size([2, 5]) tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0]])\n",
      "torch.Size([2, 3]) tensor([[-0.7853,  0.6681, -0.4439],\n",
      "        [ 1.3722,  0.5060, -0.4823]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implement using FC layer\n",
    "\n",
    "# Set hyperparams\n",
    "vocab_size = 5\n",
    "emb_dim = 3\n",
    "\n",
    "# FC layer\n",
    "fc_layer = torch.nn.Linear(vocab_size, emb_dim, bias=False)\n",
    "print(fc_layer.weight.shape, fc_layer.weight) # (emb_dim, vocab_size)\n",
    "\n",
    "# Set same weights as emb layer\n",
    "fc_layer.weight = torch.nn.Parameter(emb_layer.weight.T)  # Cast it since FloatTensor cannot be assigned to weight parameter\n",
    "print(fc_layer.weight.shape, fc_layer.weight)\n",
    "\n",
    "# Sample embedding for tokens 2, 1\n",
    "sample_token_ids = torch.tensor([2, 1])\n",
    "sample_token_onehot = torch.tensor([[0, 0, 1, 0, 0], \n",
    "                                    [0, 1, 0, 0, 0]])  # (num_tokens, vocab_size)\n",
    "#sample_token_onehot = torch.nn.functional.one_hot(sample_token_ids)\n",
    "print(sample_token_onehot.shape, sample_token_onehot)\n",
    "\n",
    "# sample_token_onehot @ fc_layer.weight.T => (num_tokens, emb_dim)\n",
    "sample_emb_fc = fc_layer(sample_token_onehot.float())  # -> 3rd, 2nd columns in fc_layer weight\n",
    "print(sample_emb_fc.shape, sample_emb_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_emb_fc == sample_emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

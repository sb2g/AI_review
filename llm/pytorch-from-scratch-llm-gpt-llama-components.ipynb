{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to implement key components of following LLM models from scratch in Pytorch to understand the concepts\n",
    "    - GPT-2\n",
    "    - LLama-2\n",
    "    - Llama-3, 3.1, 3.2\n",
    "- Some key components are:\n",
    "    - Tokenization \n",
    "        - BPE: GPT-2, Llama-3* models\n",
    "        - SentencePiece: Llama-2 model\n",
    "    - Token Embeddings layer\n",
    "        - Weight tying between input & output embedding layers: GPT-2, Llama-3.2\n",
    "    - Transformer block\n",
    "        - Attention mechanism\n",
    "            - Multi-head attention (MHA) + Causal attention (since Decoder only model)\n",
    "                - Additional optimizations?\n",
    "                    - None: GPT-2\n",
    "                    - Group query attention (GQA): Llama-3\n",
    "            - Positional embeddings\n",
    "                - Absolute, learnable: GPT-2\n",
    "                - Rotary (RoPE): Llama-2,3* models\n",
    "                    - RoPE rescaling factor / inverse frequency changes: Llama-3.1, 3.2\n",
    "        - Feedforward network (FFN)\n",
    "            - Activation function \n",
    "                - GELU: GPT-2\n",
    "                - SiLU/SwiGLU: Llama-2,3* models\n",
    "        - Activation Normalization \n",
    "            - LayerNorm: GPT-2\n",
    "                - Pre-LayerNorm: LayerNorm, MHA, Dropout --> LayerNorm, FFN, Dropout\n",
    "                - Post-LayerNorm: MHA, FFN, LayerNorm, Dropout\n",
    "            - RMSNorm: Llama-2,3* models\n",
    "        - Skip/Shortcut connections\n",
    "        - Dropout\n",
    "    - Final output layer\n",
    "        - Activation normalization\n",
    "            - LayerNorm: GPT-2\n",
    "        - Token Embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm (vs) RMSNorm\n",
    "- Why LayerNorm instead of BatchNorm: \n",
    "    - Make it robust to training batch size or seq length changes, and also easier to parallelize (no sync required across devices)\n",
    "- Why RMSNorm\n",
    "    - Only 1 learnable parameter => faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 parameters: scale, shift\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # unbiased False => No Bessel's correction => variance formula has divide by n (not n-1)\n",
    "        norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        ln = self.scale*norm + self.shift\n",
    "        return ln\n",
    "\n",
    "# 1 parameter: scale (no mean shift)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm = x * torch.rsqrt(mean + self.eps)\n",
    "        rn = self.scale*norm\n",
    "        return rn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4) # bs, seq_len, dim\n",
    "print(x)\n",
    "\n",
    "# LayerNorm\n",
    "lnorm = LayerNorm(dim=x.shape[-1], eps=1e-5)\n",
    "lnorm_pt = nn.LayerNorm(x.shape[-1], eps=1e-5)\n",
    "x_lnorm = lnorm(x)\n",
    "x_lnorm_pt = lnorm_pt(x)\n",
    "print(\"\\nLayerNorm\")\n",
    "print(x_lnorm)\n",
    "eq = torch.allclose(x_lnorm, x_lnorm_pt)\n",
    "print(eq)\n",
    "\n",
    "# RMSNorm\n",
    "rms_norm = RMSNorm(dim=x.shape[-1], eps=1e-5)\n",
    "rms_norm_pt = nn.RMSNorm(x.shape[-1], eps=1e-5)\n",
    "x_rms_norm = rms_norm(x)\n",
    "x_rms_norm_pt = rms_norm_pt(x)\n",
    "print(\"\\nRMSNorm\")\n",
    "print(x_rms_norm)\n",
    "eq = torch.allclose(x_rms_norm, x_rms_norm_pt)\n",
    "print(eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GELU (vs) SiLU\n",
    "- Rectified Linear Unit (ReLU): \n",
    "    - Used in vanilla transformers. Dying gradients for negative activations. \n",
    "- Gaussian Error Linear Unit (GELU): \n",
    "    - Used in GPT-2. Smoother\n",
    "- Sigmoid weighted Linear Unit (SiLU): \n",
    "    - More smooth. Used inside FFN of Llama-2,3* models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1./(1 + torch.exp(-x))\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "class ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.maximum(x, torch.tensor(0.0))\n",
    "    \n",
    "class LeakyReLU(nn.Module):\n",
    "    def __init__(self, negative_slope=1e-2):\n",
    "        super().__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x > 0, x, self.negative_slope * x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # approximation of x.cdf(N(x))\n",
    "        return 0.5*x*(\n",
    "                    1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)))\n",
    "                    )\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid, tanh, relu, lrelu, gelu, silu = Sigmoid(), Tanh(), ReLU(), LeakyReLU(), GELU(), SiLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_sigmoid, y_tanh, y_relu, y_lrelu, y_gelu, y_silu = sigmoid(x), tanh(x), relu(x), lrelu(x), gelu(x), silu(x)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for i, (y, label) in enumerate(zip([y_sigmoid, y_tanh, y_relu, y_lrelu, y_gelu, y_silu], [\"Sigmoid\", \"Tanh\", \"ReLU\", \"LeakyReLU\", \"GELU\", \"SiLU\"]), 1):\n",
    "    plt.subplot(1, 6, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    x = torch.randn(2, 3, 4) # bs, seq_len, dim\n",
    "    print(f\"{x}\\n\")\n",
    "\n",
    "    x_act_tuples = [(\"Sigmoid\", sigmoid(x), nn.functional.sigmoid(x)), \n",
    "                    (\"Relu\", relu(x),  nn.functional.relu(x)), \n",
    "                    (\"LeakyRelu\", lrelu(x), nn.functional.leaky_relu(x)),\n",
    "                    (\"Gelu\", gelu(x), nn.functional.gelu(x)),\n",
    "                    (\"Silu\", silu(x), nn.functional.silu(x))]\n",
    "\n",
    "    for activation_name, x_c, x_pt in x_act_tuples:\n",
    "        print(f\"{activation_name}\")\n",
    "        print(x_c)\n",
    "        eq = torch.allclose(x_c, x_pt)\n",
    "        print(f\"{eq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward layer: Regular FFN (vs) SwiGLU FFN\n",
    "- Regular FFN:\n",
    "    - Uses GELU actvn\n",
    "    - Used in Vanilla transformer, GPT-2\n",
    "- SwiGLU FFN: \n",
    "    - Applies Gated Linear Unit (GLU), i.e gate the input\n",
    "    - Uses SiLU actvn.\n",
    "    - Used in Llama-2,3* models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular FFN with GELU actvn\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4*dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# SwiGLU FFN with SiLU actvn\n",
    "class FFNSwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dtype=None, bias=False):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim, dtype=dtype, bias=bias)\n",
    "        self.fc2 = nn.Linear(dim, hidden_dim, dtype=dtype, bias=bias)\n",
    "        self.fc3 = nn.Linear(hidden_dim, dim, dtype=dtype, bias=bias)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4) # bs, seq_len, dim\n",
    "print(x)\n",
    "\n",
    "# Regular FFN\n",
    "ffn = FFN(dim=x.shape[-1])\n",
    "x_op = ffn(x)\n",
    "print(\"\\nReg FFN\")\n",
    "print(x_op)\n",
    "\n",
    "# SwiGLU FFN\n",
    "ffn_swiglu = FFNSwiGLU(dim=x.shape[-1], hidden_dim=x.shape[-1]*4)\n",
    "x_op = ffn_swiglu(x)\n",
    "print(\"\\nSwiGLU FFN\")\n",
    "print(x_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encodings & embeddings: Absolute (vs) Relative (vs) Rotary (RoPE)\n",
    "- Note: Assume model's embedding dimension is $dim$\n",
    "- Absolute positional encodings\n",
    "    - Does not explicitly encode relative position info\n",
    "    - Could be fixed embeddings or learnable embeddings\n",
    "        - Fixed sinusoidal embeddings (no parameters): \n",
    "            - Used in Vanilla transformer\n",
    "                - ${\\theta_i = \\frac{1}{10000^{\\frac{2i}{dim}}}, i \\in [0, 1, ..., \\frac{dim}{2}-1]}$\n",
    "                - $PE_{(pos,2i)} = sin(pos*\\theta_i)$, $PE_{(pos,2i + 1)} = cos(pos * \\theta_i)$\n",
    "                    - Wavelength of $sin(pos*c)$ is $\\frac{2\\pi}{c}$. \n",
    "                    - So, Wavelength of the sinusoid at PE's $2i$ th & $(2i+1)$ th dimensions is $2\\pi*{10000^{\\frac{2i}{dim}}}$ \n",
    "                    - Example wavelengths:   \n",
    "                        - At $0$, $1$ dimensions => $2\\pi$\n",
    "                        - At $2$, $3$ dimensions => $2\\pi*{10000^{\\frac{2}{dim}}}$\n",
    "                        - At ${(dim - 1)}^{th}$, ${dim}^{th}$ dimension => $2\\pi*10000$\n",
    "        - Learnable embeddings\n",
    "            - Used in GPT-2\n",
    "- Relative positional embeddings\n",
    "    - Explicitly encodes relative position info\n",
    "    - Fixed sinusoidal embeddings + Learnable relative embedding (2 parameters)\n",
    "    - Used in Transformer-XL\n",
    "- Rotary positional embeddings (RoPE)\n",
    "    - Explicitly encodes both absolute & relative position info\n",
    "    - Used in most SOTA LLMs - Llama2,3*, Mistral, etc.\n",
    "    - Fixed rotation matrix (no parameters)\n",
    "        - Idea: \n",
    "            - Rotate 2d embedding at position $m$ by $m\\theta$. \n",
    "                - $RoPE[x_{(m,1)}, x_{(m,2)}]$ =\n",
    "                \\begin{pmatrix}\n",
    "                x_{(m,1)} \\cos m\\theta - x_{(m,2)} \\sin m \\theta \\\\\n",
    "                x_{(m,2)} \\cos m\\theta + x_{(m,1)} \\sin m \\theta \\\\\n",
    "                \\end{pmatrix} \\\\  \n",
    "\n",
    "            - For 2 embeddings $x_m$ and $x_n$ at positions $m$ and $n$, \n",
    "                - Dot product of $x_m$ and $x_n$ = Dot product of $x_{m-n}$ and $x_0$\n",
    "                - So, attention on RoPE embeddings is relative\n",
    "        - Implementation: \n",
    "            - Rotate every pair of dims in input embedding $x_m$ by $m{\\theta_i}$. So $dim$ should be multiple of 2.\n",
    "                - Pair $i$ and $(i+\\frac{dim}{2})$ dimensions. \n",
    "                - ${\\theta_i = \\frac{1}{10000^{\\frac{2i}{dim}}}, i \\in [0, 1, ..., (\\frac{dim}{2}-1)]}$ \n",
    "                - for $i \\in {0, 1, ..., (\\frac{d}{2}-1)}$: \n",
    "                    - $RoPE[x_{(m,i)}, x_{(m,(i+\\frac{dim}{2}))}]$ =  \n",
    "                    \\begin{pmatrix}\n",
    "                    x_{(m,i)}  \\cos m \\theta_i - x_{(m,(i + \\frac{dim}{2}))} \\sin m \\theta_i \\\\\n",
    "                    x_{(m,(i + \\frac{dim}{2}))} \\cos m\\theta_i + x_{(m,i)} \\sin m \\theta_i \\\\\n",
    "                    \\end{pmatrix} \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute fixed positional encoding\n",
    "\n",
    "class AbsoluteFixedPositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, dim, theta_base=10000.0):\n",
    "\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, dim)\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(1).float() # seq_len\n",
    "        two_i = torch.arange(0, dim, 2, dtype=torch.float) # dim/2\n",
    "        theta = torch.exp(two_i * -(math.log(theta_base) / dim)) # dim/2\n",
    "        angles = positions * theta # (seq_len, dim/2)\n",
    "        pe[:, 0::2] = torch.sin(angles)\n",
    "        pe[:, 1::2] = torch.cos(angles)\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, dim)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, dim = x.size()\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x\n",
    "   \n",
    "theta_base = 10000.0\n",
    "seq_len = 100\n",
    "dim = 64\n",
    "pe_layer = AbsoluteFixedPositionalEncoding(seq_len, dim, theta_base)\n",
    "pe = pe_layer.pe[0] # (seq_len, dim)\n",
    "print(pe.shape)\n",
    "x = torch.randn(2, 3, dim) # bs, seq_len, dim\n",
    "x = pe_layer(x)\n",
    "print(x.shape)\n",
    "\n",
    "# Plot few dimensions\n",
    "plt.figure(figsize=(12, 2))\n",
    "positions_sin = range(0,16,8)\n",
    "positions_cos = range(1,16,8)\n",
    "plt.plot(np.arange(seq_len), pe[:seq_len, positions_sin].numpy())\n",
    "plt.plot(np.arange(seq_len), pe[:seq_len, positions_cos].numpy())\n",
    "plt.legend([\"dim %d\" % p for p in list(positions_sin) + list(positions_cos)])\n",
    "plt.xlabel(\"Sequence Position\")\n",
    "plt.ylabel(\"Embedding Dimension\")\n",
    "plt.title(\"Positional Encoding Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Plot all embeddings\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.pcolormesh(pe.T, cmap='viridis')\n",
    "plt.colorbar()\n",
    "#cax = plt.matshow(pe)\n",
    "#plt.gcf().colorbar(cax)\n",
    "plt.xlabel(\"Sequence Position\")\n",
    "plt.ylabel(\"Embedding Dimension\")\n",
    "plt.title(\"Positional Encoding Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute + Learnable positional embeddings\n",
    "class AbsoluteLearnablePositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, dim):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, dim = x.size()\n",
    "        x = x + self.pe(torch.arange(seq_len))\n",
    "        return x\n",
    "   \n",
    "seq_len = 100\n",
    "dim = 64\n",
    "pe_layer = AbsoluteLearnablePositionalEmbedding(seq_len, dim)\n",
    "pe = pe_layer.pe # (seq_len, dim)\n",
    "print(pe.weight.shape)\n",
    "x = torch.randn(2, 3, dim) # bs, seq_len, dim\n",
    "x = pe_layer(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    \n",
    "    # Relative positions embeddings (inspired from transformer-xl)\n",
    "    class RelativePositionalEmbeddings(nn.Module):\n",
    "    \n",
    "        def __init__(self, seq_len, dim, theta_base=10000):\n",
    "\n",
    "            super().__init__()\n",
    "            positions = torch.arange(seq_len - 1, -seq_len, -1.0).float()  # 2*seq_len-1\n",
    "            two_i = torch.arange(0, dim, 2.0, dtype=torch.float) # dim/2\n",
    "            theta = 1 / (theta_base ** (two_i / dim)) # dim/2\n",
    "            angles = torch.ger(positions, theta) # (2*seq_len-1, dim/2)\n",
    "            pe = torch.cat([angles.sin(), angles.cos()], dim=-1) # (2*seq_len-1, dim)            \n",
    "            self.register_buffer(\"pe\", pe)\n",
    "            self.max_len = seq_len\n",
    "\n",
    "        def forward(self, x):\n",
    "            bs, seq_len, dim = x.size()\n",
    "            zero_ix = self.max_len - 1\n",
    "            lix = zero_ix - (seq_len - 1)\n",
    "            rix = zero_ix + (seq_len - 1) + 1\n",
    "            pe = self.pe[lix:rix, :]\n",
    "            # TBD rest of it\n",
    "            return pe\n",
    "\n",
    "    theta_base = 10000.0\n",
    "    seq_len = 100\n",
    "    dim = 64\n",
    "    pe_layer = RelativePositionalEmbeddings(seq_len, dim, theta_base)\n",
    "    pe = pe_layer.pe[0] # (seq_len, dim)\n",
    "    print(pe.shape)\n",
    "    x = torch.randn(2, 3, dim) # bs, seq_len, dim\n",
    "    x = pe_layer(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotary positional embeddings (RoPE)\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, dim, theta_base=10000):\n",
    "        super().__init__()\n",
    "        positions = torch.arange(seq_len).float()\n",
    "        two_i = torch.arange(0, dim, 2, dtype=torch.float)\n",
    "        theta = 1. / (theta_base ** (two_i / dim))\n",
    "        angles = positions[:, None] * theta[None, :] # (seq_len, dim/2)\n",
    "        # or angles = torch.outer(positions, theta)  \n",
    "        # or torch.ger(positions, theta)  \n",
    "        # or angles = torch.einsum('n,d->nd', positions, theta)  \n",
    "        angles2 = torch.cat([angles, angles], dim=1) # (seq_len, dim)    \n",
    "        cos = torch.cos(angles2) # (1, 1, seq_len, dim)\n",
    "        sin = torch.sin(angles2) # (1, 1, seq_len, dim) \n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        half_dim = dim // 2\n",
    "        seq_len = x.shape[2]\n",
    "        x1 = x[:, :, :, :half_dim] # (bs, num_heads, seq_len, dim/2)\n",
    "        x2 = x[:, :, :, half_dim:] # (bs, num_heads, seq_len, dim/2)\n",
    "        neg_half_x = torch.cat([-x2, x1], dim=-1) # (bs, num_heads, seq_len, dim)\n",
    "        cos = self.cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, head_dim)\n",
    "        sin = self.sin[:seq_len, :].unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, dim)      \n",
    "        x_rope = (x * cos) + (neg_half_x * sin) # (bs, num_heads, seq_len, dim)\n",
    "        return x_rope\n",
    "\n",
    "theta_base = 10000.0\n",
    "seq_len = 100\n",
    "dim = 64\n",
    "pe_layer = RotaryPositionalEmbeddings(seq_len, dim, theta_base)\n",
    "print(pe_layer.cos.shape, pe_layer.sin.shape) # (1, 1, seq_len, dim)\n",
    "x = torch.randn(2, 1, 3, dim) # bs, num_heads, seq_len, dim\n",
    "x = pe_layer(x)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA: Regular (vs) GQA\n",
    "\n",
    "#### Regular MHA\n",
    "- For given input embedding, create q, k, v embeddings to be same size as input embedding\n",
    "- Split q, k, v embeddings into num_heads \n",
    "- Perform attention on each head\n",
    "- Compute context for each head\n",
    "- Reshape & Concatenate context results from all heads\n",
    "- Also, apply linear layer (proj) on context \n",
    "\n",
    "#### GQA\n",
    "- Multiple query heads share the same key, value. So, we can use reduced number of parameters for keys, values\n",
    "    - Lets say, 4 query heads share same key, value\n",
    "- For given input embedding (lets say dim=4096), \n",
    "    - create q embedding to be same size as input embedding\n",
    "    - create k, v embeddings to be 1/4 size as input embedding\n",
    "- Split q embeddings into num_heads groups (lets say num_heads=32)\n",
    "- Split k, v embeddings into num_heads//4 groups. (8 groups)\n",
    "    - Replicate them 4 times so that they appear as num_heads. (32 heads)\n",
    "- Perform attention on each head\n",
    "- Compute context for each head\n",
    "- Reshape & Concatenate context results from all heads\n",
    "- Also, apply linear layer (proj) on context \n",
    "> - Example:  \n",
    "> dim = 4096  \n",
    "> num_heads = 32   \n",
    "> num_kv_groups = 8   \n",
    "> head_dim = dim // num_heads = 128 \n",
    ">> - Share k, v across q  \n",
    "> q_group_size = num_heads // num_kv_groups = 32 // 8 = 4   \n",
    "> => 4 query heads share 1 k,v  \n",
    "> Wq     : dim -> num_heads     * head_dim = 4096 (32 * 128)  \n",
    "> Wk, Wv : dim -> num_kv_groups * head_dim = 1024 (8 * 128)  \n",
    "> => Reduced number of params on Wk, Wv  \n",
    "> q  = [4096] = [128, 128, ...      ]  (32 heads)  \n",
    "> k  = [1024] = [128, 128, ...] (8 parts)  \n",
    "> v  = [1024] = [128, 128, ...] (8 parts)  \n",
    ">> - Repeat each k, v params 4x for matrix mult  \n",
    "> k' = [4096] = [128, 128, ...      ]  (32 heads)  \n",
    "> v '= [4096] = [128, 128, ...      ]  (32 heads)  \n",
    ">> - Calculate attn\n",
    "> att_scores = [32 heads, seq_len, seq_len]\n",
    "> ctx = [32 heads, seq_len, 128]\n",
    ">> - Reshape & Concatenate  \n",
    "> => ctx = [seq_len, 32 * 128]  = [seq_len, 4096]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA2(nn.Module):\n",
    "    def __init__(self, din, dim, ctx_len, dropout, num_heads, bias=False, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim % num_heads == 0, \"Given dim should be multiple of num_heads\"\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads, self.din, self.dim = num_heads, din, dim\n",
    "        \n",
    "        self.wq = nn.Linear(din, dim, bias=bias, dtype=dtype)\n",
    "        self.wk = nn.Linear(din, dim, bias=bias, dtype=dtype) \n",
    "        self.wv = nn.Linear(din, dim, bias=bias, dtype=dtype)  \n",
    "        \n",
    "        att_mask = torch.triu(torch.ones(ctx_len, ctx_len), diagonal=1)\n",
    "        self.register_buffer('att_mask', att_mask)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim, bias=bias, dtype=dtype) # bias can be True here, even if qkv bias can be False\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, din = x.shape\n",
    "        \n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)  # (bs, seq_len, dim)\n",
    "\n",
    "        # Reshape to (bs, seq_len, num_heads, head_dim)\n",
    "        q = q.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.num_heads, self.head_dim) \n",
    "        v = v.view(bs, seq_len, self.num_heads, self.head_dim) \n",
    "\n",
    "        # Reshape to calculate attn in parallel for all heads\n",
    "        q = q.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # att matrix mult along seq_len, head_dim. \n",
    "        att = q @ k.transpose(2, 3) # (bs, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # causal attn + dropout \n",
    "        att_mask = self.att_mask.bool()[:seq_len, :seq_len] # Select mask for seq_len & convert to bool\n",
    "        att.masked_fill_(att_mask, -torch.inf)      \n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Calc context & reshape from (bs, num_heads, seq_len, head_dim) & then to (bs, seq_len, num_heads, head_dim)\n",
    "        ctx = (att @ v).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate heads to get (bs, seq_len, dim) & make it contiguous in memory\n",
    "        ctx = ctx.contiguous().view(bs, seq_len, self.dim)\n",
    "        ctx = self.proj(ctx)\n",
    "\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "ctx_len = 10 # Max ctx length supported\n",
    "dim = 16 # dim of att layer embeddings\n",
    "num_heads = 8\n",
    "mha_layer = MHA2(din, dim, ctx_len, 0.1, num_heads)\n",
    "ctx = mha_layer(batch_x) \n",
    "\n",
    "total_params = sum(p.numel() for p in mha_layer.parameters())\n",
    "print(f\"Number of parameters: {total_params:,}\")\n",
    "print(batch_x.shape)\n",
    "print(ctx.shape, ctx)  # bs, seqlen, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "\n",
    "    def __init__(self, din, dim, ctx_len, dropout, num_heads, num_kv_groups, bias=False, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim % num_heads == 0, \"Given dim should be multiple of num_heads\"\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads, self.din, self.dim = num_heads, din, dim\n",
    "\n",
    "        assert num_heads % num_kv_groups == 0, \"Given num_heads should be multiple of num_kv_groups\"\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups  # Number of query heads per k, v\n",
    "\n",
    "        self.wq = nn.Linear(din, dim, bias=bias, dtype=dtype)\n",
    "        self.wk = nn.Linear(din, num_kv_groups * self.head_dim, bias=bias, dtype=dtype)\n",
    "        self.wv = nn.Linear(din, num_kv_groups * self.head_dim, bias=bias, dtype=dtype)\n",
    "\n",
    "        att_mask = torch.triu(torch.ones(ctx_len, ctx_len), diagonal=1)\n",
    "        self.register_buffer('att_mask', att_mask)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim, bias=bias, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, din = x.shape\n",
    "        \n",
    "        q    = self.wq(x)               # (bs, seq_len, dim=num_heads*head_dim)\n",
    "        k, v = self.wk(x), self.wv(x)   # (bs, seq_len, num_kv_groups*head_dim)\n",
    "\n",
    "        # Reshape to (bs, seq_len, num_heads, head_dim)\n",
    "        q = q.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.num_kv_groups, self.head_dim) \n",
    "        v = v.view(bs, seq_len, self.num_kv_groups, self.head_dim) \n",
    "\n",
    "        # Reshape to calculate attn in parallel for all heads\n",
    "        q = q.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2) # (bs, num_kv_groups, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2) # (bs, num_kv_groups, seq_len, head_dim)\n",
    "\n",
    "        # Replicate k, v to match num_heads for q.\n",
    "        # E.g. groupsize = 4. [k1, k2] -> [k1, k1, k1, k1, k2, k2, k2, k2]\n",
    "        k = k.repeat_interleave(self.group_size, dim=1)  # (bs, num_heads, seq_len, head_dim)\n",
    "        v = v.repeat_interleave(self.group_size, dim=1)  # (bs, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # att matrix mult along seq_len, head_dim. \n",
    "        att = q @ k.transpose(2, 3) # (bs, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # causal attn + dropout \n",
    "        att_mask = self.att_mask.bool()[:seq_len, :seq_len] # Select mask for seq_len & convert to bool\n",
    "        att.masked_fill_(att_mask, -torch.inf)      \n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Calc context & reshape from (bs, num_heads, seq_len, head_dim) & then to (bs, seq_len, num_heads, head_dim)\n",
    "        ctx = (att @ v).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate heads to get (bs, seq_len, dim) & make it contiguous in memory\n",
    "        #ctx = ctx.contiguous().view(bs, seq_len, self.dim)\n",
    "        ctx = ctx.reshape(bs, seq_len, self.dim)\n",
    "        ctx = self.proj(ctx)\n",
    "\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "ctx_len = 10 # Max ctx length supported\n",
    "dim = 16 # dim of att layer embeddings\n",
    "num_heads = 8\n",
    "num_kv_groups = 2 # i.e. 4 q heads share 1 k,v \n",
    "gqa_layer = GQA(din, dim, ctx_len, 0.1, num_heads, num_kv_groups)\n",
    "ctx = gqa_layer(batch_x) \n",
    "\n",
    "total_params = sum(p.numel() for p in gqa_layer.parameters())\n",
    "print(f\"Number of parameters: {total_params:,}\")\n",
    "print(batch_x.shape)\n",
    "print(ctx.shape, ctx)  # bs, seqlen, dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rope + GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "\n",
    "    def __init__(self, din, dim, ctx_len, dropout, num_heads, num_kv_groups, bias=False, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim % num_heads == 0, \"Given dim should be multiple of num_heads\"\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads, self.din, self.dim = num_heads, din, dim\n",
    "\n",
    "        assert num_heads % num_kv_groups == 0, \"Given num_heads should be multiple of num_kv_groups\"\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups  # Number of query heads per k, v\n",
    "\n",
    "        self.wq = nn.Linear(din, dim, bias=bias, dtype=dtype)\n",
    "        self.wk = nn.Linear(din, num_kv_groups * self.head_dim, bias=bias, dtype=dtype)\n",
    "        self.wv = nn.Linear(din, num_kv_groups * self.head_dim, bias=bias, dtype=dtype)\n",
    "\n",
    "        att_mask = torch.triu(torch.ones(ctx_len, ctx_len), diagonal=1)\n",
    "        self.register_buffer('att_mask', att_mask)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim, bias=bias, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, din = x.shape\n",
    "        \n",
    "        q    = self.wq(x)               # (bs, seq_len, dim=num_heads*head_dim)\n",
    "        k, v = self.wk(x), self.wv(x)   # (bs, seq_len, num_kv_groups*head_dim)\n",
    "\n",
    "        # Reshape to (bs, seq_len, num_heads, head_dim)\n",
    "        q = q.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.num_kv_groups, self.head_dim) \n",
    "        v = v.view(bs, seq_len, self.num_kv_groups, self.head_dim) \n",
    "\n",
    "        # Reshape to calculate attn in parallel for all heads\n",
    "        q = q.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2) # (bs, num_kv_groups, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2) # (bs, num_kv_groups, seq_len, head_dim)\n",
    "\n",
    "        # Replicate k, v to match num_heads for q.\n",
    "        # E.g. groupsize = 4. \n",
    "        # [k1, k2] -> [k1, k1, k1, k1, k2, k2, k2, k2]\n",
    "        # [v1, v2] -> [v1, v1, v1, v1, v2, v2, v2, v2]\n",
    "        k = k.repeat_interleave(self.group_size, dim=1)  # (bs, num_heads, seq_len, head_dim)\n",
    "        v = v.repeat_interleave(self.group_size, dim=1)  # (bs, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # att matrix mult along seq_len, head_dim. \n",
    "        att = q @ k.transpose(2, 3) # (bs, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # causal attn + dropout \n",
    "        att_mask = self.att_mask.bool()[:seq_len, :seq_len] # Select mask for seq_len & convert to bool\n",
    "        att.masked_fill_(att_mask, -torch.inf)      \n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Calc context & reshape from (bs, num_heads, seq_len, head_dim) & then to (bs, seq_len, num_heads, head_dim)\n",
    "        ctx = (att @ v).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate heads to get (bs, seq_len, dim) & make it contiguous in memory\n",
    "        #ctx = ctx.contiguous().view(bs, seq_len, self.dim)\n",
    "        ctx = ctx.reshape(bs, seq_len, self.dim)\n",
    "        ctx = self.proj(ctx)\n",
    "\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "ctx_len = 10 # Max ctx length supported\n",
    "dim = 16 # dim of att layer embeddings\n",
    "num_heads = 8\n",
    "num_kv_groups = 2 # i.e. 4 q heads share 1 k,v \n",
    "gqa_layer = GQA(din, dim, ctx_len, 0.1, num_heads, num_kv_groups)\n",
    "ctx = gqa_layer(batch_x) \n",
    "\n",
    "total_params = sum(p.numel() for p in gqa_layer.parameters())\n",
    "print(f\"Number of parameters: {total_params:,}\")\n",
    "print(batch_x.shape)\n",
    "print(ctx.shape, ctx)  # bs, seqlen, dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class x(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class y(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "> https://github.com/labmlai/annotated_deep_learning_paper_implementations\n",
    "\n",
    "> https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77b435ab4390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple NN model for demo\n",
    "class SimpleModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, dim)\n",
    "        self.linear = torch.nn.Linear(dim, dim)\n",
    "        self.lm_head = torch.nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.emb(input_ids)\n",
    "        x = self.linear(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "# Define a vocab\n",
    "vocab = [\n",
    "    \"red\",\n",
    "    \"orange\",\n",
    "    \"yellow\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"indigo\",\n",
    "    \"violet\",\n",
    "    \"magenta\",\n",
    "    \"marigold\",\n",
    "    \"chartreuse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "vocab_size, dim, num_classes = 5, 1024, 10\n",
    "simple_model = SimpleModel(vocab_size, dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) tensor([[2, 4, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input \n",
    "sample_token_ids = torch.LongTensor([[2, 4, 0]])\n",
    "print(sample_token_ids.shape, sample_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate next token\n",
    "def generate_next_token(model, vocab, **kwargs):\n",
    "    \n",
    "    # Generate next token id\n",
    "    with torch.no_grad():\n",
    "        logits = model(**kwargs)\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_ids = next_token_logits.argmax(dim=1)  # Argmax on vocab\n",
    "    print(next_token_ids)\n",
    "\n",
    "    # Get next token\n",
    "    next_tokens = [vocab[token_id] for token_id in next_token_ids]\n",
    "    print(next_tokens)\n",
    "\n",
    "    return next_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8])\n",
      "['marigold']\n"
     ]
    }
   ],
   "source": [
    "# Generate next token using base model\n",
    "next_token = generate_next_token(simple_model, vocab, input_ids=sample_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024]) torch.Size([1, 3, 1024]) torch.Size([1, 3, 1024])\n",
      "LoRA trainable params: 4096, Actual params: 1048576\n"
     ]
    }
   ],
   "source": [
    "# LoRA concept on linear layer\n",
    "\n",
    "# Example linear layer input\n",
    "seq_len = 3\n",
    "dim = 1024\n",
    "x = torch.randn(1, seq_len, dim) # (bs, seq_len, dim)\n",
    "\n",
    "# Example lora_a, lora_b\n",
    "rank = 2\n",
    "A = torch.randn(dim, rank)\n",
    "B = torch.randn(rank, dim)\n",
    "\n",
    "# Concept\n",
    "base_output = simple_model.linear(x)\n",
    "lora_output = x @ A @ B\n",
    "total_output = base_output + lora_output\n",
    "print(base_output.shape, lora_output.shape, total_output.shape)\n",
    "\n",
    "# Num elems\n",
    "base_num_params = simple_model.linear.weight.numel()\n",
    "lora_num_params = A.numel() + B.numel()\n",
    "print(f\"LoRA trainable params: {lora_num_params}, Actual params: {base_num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LoRA layer in Pytorch\n",
    "\n",
    "# Main LoRA\n",
    "import math\n",
    "class LoraLayerModule(torch.nn.Module):\n",
    "    def __init__(self, din, dout, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.A = torch.nn.Parameter(torch.empty(din, rank)) # torch.randn(din, rank)\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, dout)) # torch.randn(rank, dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "# LoRA on any base layer\n",
    "class LoraLayer(torch.nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        din, dout = base_layer.weight.shape\n",
    "        self.lora = LoraLayerModule(din, dout, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_layer(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Test Lora layer\n",
    "rank, alpha = 2, 4\n",
    "lora_layer = LoraLayer(simple_model.linear, rank, alpha)\n",
    "\n",
    "# Example linear layer input\n",
    "seq_len = 3\n",
    "dim = 1024\n",
    "x = torch.randn(1, seq_len, dim) # (bs, seq_len, dim)\n",
    "total_output = lora_layer(x)\n",
    "print(total_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (emb): Embedding(5, 1024)\n",
      "  (linear): LoraLayer(\n",
      "    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (lora): LoraLayerModule()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simple way to create lora layer & replace existing model layers\n",
    "\n",
    "# Create a base model\n",
    "vocab_size, dim, num_classes = 5, 1024, 10\n",
    "simple_model = SimpleModel(vocab_size, dim, num_classes)\n",
    "\n",
    "# Create Lora layer\n",
    "rank, alpha = 2, 4\n",
    "lora_layer = LoraLayer(simple_model.linear, rank, alpha)\n",
    "\n",
    "# Replace existing with Lora\n",
    "simple_model.linear = lora_layer\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n",
      "['chartreuse']\n"
     ]
    }
   ],
   "source": [
    "# Generate next token using Lora model\n",
    "next_token = generate_next_token(simple_model, vocab, input_ids=sample_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num trainable parameters before freeze: 1,064,970\n",
      "Num trainable parameters after freeze: 0\n",
      "['linear']\n",
      "SimpleModel(\n",
      "  (emb): Embedding(5, 1024)\n",
      "  (linear): LoraLayer(\n",
      "    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (lora): LoraLayerModule()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "Num Lora trainable parameters: 4,096\n"
     ]
    }
   ],
   "source": [
    "# General way to create lora layer & replace existing model layers\n",
    "\n",
    "# Get modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "# Replace layers with Lora\n",
    "def replace_layers_with_lora(model, replace_layers, rank, alpha):\n",
    "    for name, module in model.named_modules():\n",
    "        name = name.split('.')\n",
    "        name = name[0] if len(name) == 1 else name[-1]\n",
    "        if name in replace_layers:\n",
    "            setattr(model, name, LoraLayer(module, rank, alpha))\n",
    "\n",
    "# Get trainable params\n",
    "def get_num_trainable_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "# Create a base model\n",
    "vocab_size, dim, num_classes = 5, 1024, 10\n",
    "simple_model = SimpleModel(vocab_size, dim, num_classes)\n",
    "\n",
    "# Freeze all params\n",
    "print(f\"Num trainable parameters before freeze: {get_num_trainable_params(simple_model):,}\")\n",
    "for param in simple_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f\"Num trainable parameters after freeze: {get_num_trainable_params(simple_model):,}\")\n",
    "\n",
    "# Find layers for Lora\n",
    "replace_layers = find_all_linear_names(simple_model)\n",
    "print(replace_layers)\n",
    "\n",
    "# LoRA params & layers\n",
    "rank, alpha = 2, 4\n",
    "replace_layers_with_lora(simple_model, replace_layers, rank, alpha)\n",
    "print(simple_model)\n",
    "print(f\"Num Lora trainable parameters: {get_num_trainable_params(simple_model):,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "['magenta']\n"
     ]
    }
   ],
   "source": [
    "# Generate next token using Lora model\n",
    "next_token = generate_next_token(simple_model, vocab, input_ids=sample_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> https://www.deeplearning.ai/short-courses/efficiently-serving-llms/\n",
    "\n",
    "> https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "> https://www.coursera.org/specializations/generative-ai-engineering-with-llms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

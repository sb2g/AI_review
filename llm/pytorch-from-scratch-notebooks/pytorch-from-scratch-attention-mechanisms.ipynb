{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bff2110e390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [[0.11, 0.12, 0.13, 0.14],\n",
    "     [0.21, 0.22, 0.23, 0.24],\n",
    "     [0.31, 0.32, 0.33, 0.34]]\n",
    ")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
       "         [0.0740, 0.8665, 0.1366, 0.1025],\n",
       "         [0.1841, 0.7264, 0.3153, 0.6871]]),\n",
       " tensor([[-0.9724, -0.7550,  0.3239, -0.1085],\n",
       "         [ 0.2103, -0.3908,  0.2350,  0.6653],\n",
       "         [ 0.3528,  0.9728, -0.0386, -0.8861]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,4), torch.zeros(x.shape), torch.ones(x.shape), torch.rand(x.shape), torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3), diagonal=1), torch.triu(torch.ones(3,3), diagonal=1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4709, -0.4269, -0.0283,  1.4220],\n",
      "        [-0.3886, -0.8903, -0.9601, -0.4087],\n",
      "        [ 1.0764, -0.4015, -0.7291, -0.1218]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9496, -0.8609, -0.0571,  2.8676],\n",
       "        [ 0.1468,  0.3363,  0.3626,  0.1544],\n",
       "        [-6.1105,  2.2795,  4.1392,  0.6917]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to normalize x along each row\n",
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "x / x.sum(dim=1, keepdim=True) # or.view(x.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7915)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.7915)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "res = 0\n",
    "for ix in range(len(x[0])):\n",
    "    res += x[0][ix] * x[1][ix]\n",
    "print(res)\n",
    "\n",
    "torch.dot(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.0545, -1.7496,  1.3280],\n",
      "        [-1.7496,  1.5781,  0.5650],\n",
      "        [ 1.3280,  0.5650,  1.5604]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0545, -1.7496,  1.3280],\n",
       "        [-1.7496,  1.5781,  0.5650],\n",
       "        [ 1.3280,  0.5650,  1.5604]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix dot product\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "att = torch.empty(len(x), len(x))\n",
    "for ix1 in range(len(x)):\n",
    "    for ix2 in range(len(x)):\n",
    "        att[ix1, ix2] = torch.dot(x[ix1], x[ix2])\n",
    "print(att)\n",
    "\n",
    "x @ x.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9660e-01, 1.4961e-04, 3.2474e-03],\n",
      "        [2.5644e-02, 7.1482e-01, 2.5953e-01],\n",
      "        [3.6658e-01, 1.7092e-01, 4.6250e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Softmax \n",
    "\n",
    "def softmax_custom(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True) # or .view(x.shape[0], 1)\n",
    "print(softmax_custom(att))\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "att = x @ x.T\n",
    "\n",
    "att_w = torch.softmax(att, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "tensor([[ 0.4221,  0.0596, -0.3723, -0.7592],\n",
      "        [ 0.5339,  0.0740, -0.6985, -0.6406],\n",
      "        [ 0.9379,  0.1913, -0.3909,  1.0240]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4221,  0.0596, -0.3723, -0.7592],\n",
       "        [ 0.5339,  0.0740, -0.6985, -0.6406],\n",
       "        [ 0.9379,  0.1913, -0.3909,  1.0240]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contextual vectors: Multiply att_w with x\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "att = x @ x.T\n",
    "att_w = torch.softmax(att, dim=-1)\n",
    "print(att_w.shape, x.shape)\n",
    "\n",
    "ctx = torch.zeros(*x.shape)\n",
    "print(ctx.shape)\n",
    "for ix1 in range(x.shape[0]):\n",
    "    for ix2 in range(att_w.shape[1]):\n",
    "        ctx[ix1] += att_w[ix1, ix2] * x[ix2]\n",
    "print(ctx)\n",
    "\n",
    "ctx = att_w @ x\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention - Basic\n",
    "- No trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([3, 3]) torch.Size([3, 4])\n",
      "tensor([[-0.9471,  0.4830, -0.2339, -0.9257],\n",
      "        [-0.9380,  0.8607, -0.7171, -1.3901],\n",
      "        [ 0.6743, -0.2244,  0.5628,  0.9100]])\n"
     ]
    }
   ],
   "source": [
    "# In short\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "att = x @ x.T\n",
    "att = torch.softmax(att, dim=-1)\n",
    "ctx = att @ x\n",
    "print(x.shape, att.shape, ctx.shape)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention - Option 1\n",
    "- With trainable weight parameters\n",
    "- Using torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) tensor([[-0.6417, -0.0450, -1.1361],\n",
      "        [-1.2040, -0.1568, -2.0606],\n",
      "        [ 0.0228,  0.0875, -0.0442]])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttn1(nn.Module):\n",
    "    def __init__(self, din, d):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Parameter(torch.rand(din, d), requires_grad=False)\n",
    "        self.wk = nn.Parameter(torch.rand(din, d), requires_grad=False) \n",
    "        self.wv = nn.Parameter(torch.rand(din, d), requires_grad=False)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = x @ self.wq, x @ self.wk, x @ self.wv\n",
    "        att = q @ k.T\n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        ctx = att @ v\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "din = x.shape[1]\n",
    "d = 3\n",
    "sa_layer = SelfAttn1(din, d)\n",
    "ctx = sa_layer(x)\n",
    "print(ctx.shape, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention - Option 2\n",
    "- With trainable weight parameters\n",
    "- Using torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) tensor([[-0.3736,  0.3227,  0.0807],\n",
      "        [ 0.1649,  0.1902,  0.4977],\n",
      "        [ 0.0631,  0.2223,  0.4000]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Linear has better weight initialization\n",
    "\n",
    "class SelfAttn2(nn.Module):\n",
    "    def __init__(self, din, d, bias=False):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(din, d, bias=bias)\n",
    "        self.wk = nn.Linear(din, d, bias=bias) \n",
    "        self.wv = nn.Linear(din, d, bias=bias)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        att = q @ k.T\n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        ctx = att @ v\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "din = x.shape[1]\n",
    "d = 3\n",
    "sa_layer = SelfAttn2(din, d)\n",
    "ctx = sa_layer(x)\n",
    "print(ctx.shape, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[-0.4432,  0.1943,  0.0184],\n",
      "        [-0.4422,  0.0369,  0.0991],\n",
      "        [ 1.4514, -0.1574, -0.8511]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.2667, 0.3853, 0.3481],\n",
      "        [0.2713, 0.3578, 0.3709],\n",
      "        [0.6025, 0.2380, 0.1595]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self attn forward internals for experiments below\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "q, k, v = sa_layer.wq(x), sa_layer.wk(x), sa_layer.wv(x)\n",
    "att_orig = q @ k.T\n",
    "att = torch.softmax( att_orig / k.shape[-1]**0.5, dim=-1)\n",
    "ctx = att @ v\n",
    "\n",
    "print(att.shape)\n",
    "print(att_orig)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "Dropout scales non dropout nodes by: 1.25\n",
      "tensor([[1.2500, 1.2500, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 0.0000, 1.2500]])\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "y = torch.ones(4, 4)\n",
    "print(y)\n",
    "\n",
    "print(\"Dropout scales non dropout nodes by:\", 1/(1-0.2))\n",
    "\n",
    "print(dropout(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Causal attention - Option 1\n",
    "- Apply mask after Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.2667, 0.0000, 0.0000],\n",
      "        [0.2713, 0.3578, 0.0000],\n",
      "        [0.6025, 0.2380, 0.1595]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4313, 0.5687, 0.0000],\n",
      "        [0.6025, 0.2380, 0.1595]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att_len = att.shape[0]\n",
    "att_mask = torch.tril(torch.ones(att_len, att_len))\n",
    "att = att * att_mask\n",
    "print(att_mask)\n",
    "print(att)\n",
    "\n",
    "# Need to normalize again\n",
    "att = att / att.sum(dim=-1, keepdim=True)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal attention - Option 2 (preferred)\n",
    "- Apply mask before Softmax (no need to re-normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[-0.4432,    -inf,    -inf],\n",
      "        [-0.4422,  0.0369,    -inf],\n",
      "        [ 1.4514, -0.1574, -0.8511]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4313, 0.5687, 0.0000],\n",
      "        [0.6025, 0.2380, 0.1595]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att_len = att_orig.shape[0]\n",
    "att_mask = torch.triu(torch.ones(att_len, att_len), diagonal=1)\n",
    "att = att_orig.masked_fill(att_mask.bool(), -torch.inf)\n",
    "print(att_mask)\n",
    "print(att)\n",
    " \n",
    "# Apply softmax (first time)\n",
    "att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "print(att)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal attention - Support batch inputs + Pytorch buffers \n",
    "- Batch inputs on dimension 0\n",
    "- Using Pytorch buffers for attention mask\n",
    "    - Saved in model state_dict()\n",
    "    - Also gets transferred to device along with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) tensor([[[-0.4459,  0.8970, -2.0975],\n",
      "         [-0.6559, -0.2561, -0.6735],\n",
      "         [-0.6442, -0.5407, -0.2109]],\n",
      "\n",
      "        [[-0.4459,  0.8970, -2.0975],\n",
      "         [-0.6559, -0.2561, -0.6735],\n",
      "         [-0.6442, -0.5407, -0.2109]]], grad_fn=<UnsafeViewBackward0>)\n",
      "OrderedDict([('att_mask', tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])), ('wq.weight', tensor([[ 0.1647,  0.4296, -0.1152,  0.4357],\n",
      "        [-0.2384, -0.0656,  0.3323, -0.2590],\n",
      "        [ 0.3815,  0.1226, -0.0098,  0.4279]])), ('wk.weight', tensor([[ 0.3751, -0.2057,  0.0485,  0.0583],\n",
      "        [ 0.4096,  0.2810,  0.4049,  0.3048],\n",
      "        [-0.4351,  0.3322, -0.1328,  0.4012]])), ('wv.weight', tensor([[ 0.3146, -0.2923, -0.0526,  0.0746],\n",
      "        [ 0.1429, -0.4631,  0.0224,  0.2605],\n",
      "        [ 0.2823,  0.2459,  0.0791, -0.4796]]))])\n",
      "<class 'torch.Tensor'> cpu\n"
     ]
    }
   ],
   "source": [
    "class CausalAttn(nn.Module):\n",
    "    def __init__(self, din, dim, seq_len, dropout, bias=False):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(din, dim, bias=bias)\n",
    "        self.wk = nn.Linear(din, dim, bias=bias) \n",
    "        self.wv = nn.Linear(din, dim, bias=bias)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # att mask as regular tensor\n",
    "        # self.att_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # att mask as pytorch buffer\n",
    "        att_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        self.register_buffer('att_mask', att_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, din = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        att = q @ k.transpose(1, 2)\n",
    "        att_mask = self.att_mask.bool()[:seq_len, :seq_len] # Select mask for seq_len & bool\n",
    "        att.masked_fill_(att_mask, -torch.inf)      \n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        ctx = att @ v\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "dim = 3 # dim of att layer embeddings\n",
    "seq_len = 6 # Max ctx length supported\n",
    "ca_layer = CausalAttn(din, dim, seq_len, 0.1)\n",
    "ctx = ca_layer(batch_x)\n",
    "\n",
    "print(ctx.shape, ctx)\n",
    "print(ca_layer.state_dict())\n",
    "print(type(ca_layer.att_mask), ca_layer.att_mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('att_mask', tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])), ('wq.weight', tensor([[ 0.1647,  0.4296, -0.1152,  0.4357],\n",
      "        [-0.2384, -0.0656,  0.3323, -0.2590],\n",
      "        [ 0.3815,  0.1226, -0.0098,  0.4279]])), ('wk.weight', tensor([[ 0.3751, -0.2057,  0.0485,  0.0583],\n",
      "        [ 0.4096,  0.2810,  0.4049,  0.3048],\n",
      "        [-0.4351,  0.3322, -0.1328,  0.4012]])), ('wv.weight', tensor([[ 0.3146, -0.2923, -0.0526,  0.0746],\n",
      "        [ 0.1429, -0.4631,  0.0224,  0.2605],\n",
      "        [ 0.2823,  0.2459,  0.0791, -0.4796]]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1325840/764423285.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ca_layer.load_state_dict(torch.load('./output_dir/ca_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(ca_layer.state_dict(), '../output_dir/ca_model.pth')\n",
    "\n",
    "# Load model & check if buffer is stored\n",
    "ca_layer = CausalAttn(din, dim, seq_len, 0.1)\n",
    "ca_layer.load_state_dict(torch.load('../output_dir/ca_model.pth'))\n",
    "print(ca_layer.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention - Option 1\n",
    "- Do multi-head attn calculations serially  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 8]) tensor([[[ 1.3275e-01,  1.8884e-02, -1.1431e+00,  5.8095e-01, -5.4552e-01,\n",
      "          -9.2739e-01, -1.3008e-01, -6.0687e-02],\n",
      "         [ 8.8851e-02, -3.9358e-03, -2.8078e-02, -2.2807e-01,  1.0267e-01,\n",
      "          -3.2266e-05, -1.3571e-01,  1.2448e-01],\n",
      "         [-2.8530e-02, -1.4310e-01, -4.4194e-01,  2.7705e-01, -7.6101e-02,\n",
      "          -2.7175e-01, -1.4880e-01,  8.3744e-02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4552e-01,\n",
      "          -9.2739e-01, -1.3008e-01, -6.0687e-02],\n",
      "         [ 1.5331e-01,  5.2337e-03, -5.8312e-01,  5.4018e-02, -1.8933e-01,\n",
      "          -4.9643e-01, -2.0533e-01,  9.1993e-02],\n",
      "         [-2.8530e-02, -1.4310e-01, -4.4194e-01,  2.7705e-01, -2.9850e-02,\n",
      "           5.4718e-02,  1.1047e-01,  2.9939e-02]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MHA1(nn.Module):\n",
    "    def __init__(self, din, dim, seq_len, dropout, num_heads, bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttn(din, dim, seq_len, dropout, bias) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        ctx = torch.cat(heads, dim=-1)\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "dim = 4 # dim of att layer embeddings\n",
    "seq_len = 10 # Max ctx length supported\n",
    "num_heads = 2\n",
    "ca_layer = MHA1(din, dim, seq_len, 0.1, num_heads)\n",
    "ctx = ca_layer(batch_x) \n",
    "\n",
    "print(batch_x.shape)\n",
    "print(ctx.shape, ctx)  # bs, seqlen, num_heads*dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention - Option 2 (preferred) \n",
    "- Do multi-head attn calculations in parallel \n",
    "- For given input embedding, create q, k, v embeddings to be same size as input embedding\n",
    "- Split q, k, v embeddings into num_heads \n",
    "- Perform attention on each head\n",
    "- Compute context for each head\n",
    "- Reshape & Concatenate context results from all heads\n",
    "- Also, apply linear layer (proj) on context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 4]) tensor([[[ 0.1145,  0.3370,  0.1083,  0.1875],\n",
      "         [-0.0768,  0.1804, -0.0021,  0.2917],\n",
      "         [-0.3022,  0.2157, -0.0712,  0.3215]],\n",
      "\n",
      "        [[ 0.1145,  0.3370,  0.1083,  0.1875],\n",
      "         [-0.0768,  0.1804, -0.0021,  0.2917],\n",
      "         [-0.2115,  0.1560, -0.0558,  0.3222]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MHA2(nn.Module):\n",
    "    def __init__(self, din, dim, seq_len, dropout, num_heads, bias=False, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim % num_heads == 0, \"Given dim should be multiple of num_heads\"\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_heads, self.din, self.dim = num_heads, din, dim\n",
    "        \n",
    "        self.wq = nn.Linear(din, dim, bias=bias, dtype=dtype)\n",
    "        self.wk = nn.Linear(din, dim, bias=bias, dtype=dtype) \n",
    "        self.wv = nn.Linear(din, dim, bias=bias, dtype=dtype)  \n",
    "        \n",
    "        att_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        self.register_buffer('att_mask', att_mask)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim, bias=bias, dtype=dtype) # bias can be True here, even if qkv bias can be False\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, din = x.shape\n",
    "        \n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)  # (bs, seq_len, dim)\n",
    "\n",
    "        # Reshape to (bs, seq_len, num_heads, head_dim)\n",
    "        q = q.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.num_heads, self.head_dim) \n",
    "        v = v.view(bs, seq_len, self.num_heads, self.head_dim) \n",
    "\n",
    "        # Reshape to calculate attn in parallel for all heads\n",
    "        q = q.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2) # (bs, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # att matrix mult along seq_len, head_dim. \n",
    "        att = q @ k.transpose(2, 3) # (bs, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # causal attn + dropout \n",
    "        att_mask = self.att_mask.bool()[:seq_len, :seq_len] # Select mask for seq_len & convert to bool\n",
    "        att.masked_fill_(att_mask, -torch.inf)      \n",
    "        att = torch.softmax(att / k.shape[-1]**0.5, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Calc context & reshape from (bs, num_heads, seq_len, head_dim) & then to (bs, seq_len, num_heads, head_dim)\n",
    "        ctx = (att @ v).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate heads to get (bs, seq_len, dim) & make it contiguous in memory\n",
    "        ctx = ctx.contiguous().view(bs, seq_len, self.dim)\n",
    "        ctx = self.proj(ctx)\n",
    "\n",
    "        return ctx\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "batch_x = torch.stack((x, x), dim=0) # Create a batch of input\n",
    "din = batch_x.shape[2] # Input dim\n",
    "dim = 4 # dim of att layer embeddings\n",
    "seq_len = 10 # Max ctx length supported\n",
    "num_heads = 2\n",
    "mha_layer = MHA2(din, dim, seq_len, 0.1, num_heads)\n",
    "ctx = mha_layer(batch_x) \n",
    "\n",
    "print(batch_x.shape)\n",
    "print(ctx.shape, ctx)  # bs, seqlen, dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "> https://github.com/rasbt/LLMs-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

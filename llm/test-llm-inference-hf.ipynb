{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is this notebook about?\n",
    "- This notebook shows how to run inference on any LLM using huggingface. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbujimal/miniforge3/envs/mlenv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "#model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "#model_name = \"Qwen/Qwen-Audio\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_name, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    ")\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Auto classes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    #max_length=256,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.05,\n",
    "    do_sample=True,\n",
    "    #do_sample=False,\n",
    "    use_cache=True,\n",
    "    skip_special_tokens=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "def tokenize_generate_response(tokenizer, model, generation_config, messages):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    full_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"Full response (including prompt):\")\n",
    "    print(full_response)\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"Generated response:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response (including prompt):\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Mar 2025\n",
      "\n",
      "You are a helpful assistant.user\n",
      "\n",
      "Give me a short introduction to large language model.assistant\n",
      "\n",
      "**Introduction to Large Language Models**\n",
      "\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data, enabling them to learn patterns, relationships, and structures of language. The goal of LLMs is to generate human-like text, answer questions, and complete tasks that require language understanding.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "1. **Training Data**: LLMs are trained on massive amounts of text data, often sourced from the internet, books, and other digital sources.\n",
      "2. **Neural Network Architecture**: LLMs are built using neural networks, which are composed of multiple layers of interconnected nodes (neurons) that process and transform input data.\n",
      "3. **Self-Supervised Learning**: LLMs learn from unlabeled data, where the model is trained to predict the next word or sequence of words in a given context.\n",
      "4. **Generative Capabilities**: LLMs can generate text, answer questions, and complete tasks, such as language translation, text summarization, and text classification.\n",
      "\n",
      "**Types of Large Language Models:**\n",
      "\n",
      "1. **Transformer Models**: Introduced in 2017, transformer models use self-attention mechanisms to process input sequences\n",
      "Generated response:\n",
      "**Introduction to Large Language Models**\n",
      "\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data, enabling them to learn patterns, relationships, and structures of language. The goal of LLMs is to generate human-like text, answer questions, and complete tasks that require language understanding.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "1. **Training Data**: LLMs are trained on massive amounts of text data, often sourced from the internet, books, and other digital sources.\n",
      "2. **Neural Network Architecture**: LLMs are built using neural networks, which are composed of multiple layers of interconnected nodes (neurons) that process and transform input data.\n",
      "3. **Self-Supervised Learning**: LLMs learn from unlabeled data, where the model is trained to predict the next word or sequence of words in a given context.\n",
      "4. **Generative Capabilities**: LLMs can generate text, answer questions, and complete tasks, such as language translation, text summarization, and text classification.\n",
      "\n",
      "**Types of Large Language Models:**\n",
      "\n",
      "1. **Transformer Models**: Introduced in 2017, transformer models use self-attention mechanisms to process input sequences\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "tokenize_generate_response(tokenizer, model, generation_config, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

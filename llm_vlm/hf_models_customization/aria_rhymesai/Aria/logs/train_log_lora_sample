[2025-06-12 19:38:26,686] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
`grouped_gemm` is not installed, using sequential GEMM, which is slower.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]
Loading checkpoint shards:   8%|▊         | 1/12 [00:00<00:03,  2.99it/s]
Loading checkpoint shards:  17%|█▋        | 2/12 [00:00<00:02,  3.66it/s]
Loading checkpoint shards:  25%|██▌       | 3/12 [00:00<00:02,  3.92it/s]
Loading checkpoint shards:  33%|███▎      | 4/12 [00:01<00:01,  4.08it/s]
Loading checkpoint shards:  42%|████▏     | 5/12 [00:01<00:01,  4.25it/s]
Loading checkpoint shards:  50%|█████     | 6/12 [00:01<00:01,  4.48it/s]
Loading checkpoint shards:  58%|█████▊    | 7/12 [00:01<00:01,  4.55it/s]
Loading checkpoint shards:  67%|██████▋   | 8/12 [00:01<00:00,  4.75it/s]
Loading checkpoint shards:  75%|███████▌  | 9/12 [00:02<00:00,  4.80it/s]
Loading checkpoint shards:  83%|████████▎ | 10/12 [00:02<00:00,  4.97it/s]
Loading checkpoint shards:  92%|█████████▏| 11/12 [00:02<00:00,  5.21it/s]
Loading checkpoint shards: 100%|██████████| 12/12 [00:02<00:00,  5.41it/s]
Loading checkpoint shards: 100%|██████████| 12/12 [00:02<00:00,  4.66it/s]
/home/user/Aria/venva3/lib/python3.10/site-packages/peft/tuners/lora/layer.py:95: UserWarning: Unsupported layer type '<class 'aria.model.moe_lm.GroupedGEMM'>' encountered, proceed at your own risk.
  warnings.warn(
Using auto half precision backend
***** Running training *****
  Num examples = 21
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 1
  Number of trainable parameters = 154,333,184
trainable params: 154,333,184 || all params: 25,461,642,096 || trainable%: 0.6061
datasets/nextqa

  0%|          | 0/1 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)

100%|██████████| 1/1 [00:42<00:00, 42.43s/it]
                                             

100%|██████████| 1/1 [00:42<00:00, 42.43s/it]Saving model checkpoint to outputs_lora/checkpoint-1
tokenizer config file saved in outputs_lora/checkpoint-1/tokenizer_config.json
Special tokens file saved in outputs_lora/checkpoint-1/special_tokens_map.json
Saving model checkpoint to outputs_lora/checkpoint-1
tokenizer config file saved in outputs_lora/checkpoint-1/tokenizer_config.json
Special tokens file saved in outputs_lora/checkpoint-1/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)



                                             

100%|██████████| 1/1 [00:50<00:00, 42.43s/it]
100%|██████████| 1/1 [00:50<00:00, 50.46s/it]
Image processor saved in outputs_lora/preprocessor_config.json
Saving model checkpoint to outputs_lora/
tokenizer config file saved in outputs_lora/tokenizer_config.json
Special tokens file saved in outputs_lora/special_tokens_map.json
{'loss': 0.594, 'grad_norm': 6.0625, 'learning_rate': 5e-05, 'epoch': 0.67}
{'train_runtime': 50.4685, 'train_samples_per_second': 0.416, 'train_steps_per_second': 0.02, 'train_loss': 0.5939578413963318, 'epoch': 0.67}

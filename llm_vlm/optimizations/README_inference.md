# Overview:
- LLM/LVLMs have billions of parameters. So, they can consume significant memory & latency during inference.
- Special techniques are used to optimize them for inference:
    - Batching
    - Parallelization
    - Attention mechanisms
        - MQA / GQA
        - SDPA
        - Flash attention
        - Paged attention
    - KV caching
    - Model optimization techinques
        - Quantization
        - Pruning
        - Distillation
    - Model serving
        - Speculative decoding

# Compute requirements of Transformer models
- 

# Quantization
- 

# References
- https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
- https://huggingface.co/docs/transformers/v4.53.0/quantization/overview
- EfficientML.ai Course: https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB
- https://horace.io/brrr_intro.html
- https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization


2025-05-21 10:19:49.996006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1747822790.012446 3136852 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1747822790.017559 3136852 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1747822790.031929 3136852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1747822790.031949 3136852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1747822790.031951 3136852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1747822790.031953 3136852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-21 10:19:50.036231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Training set length: 935
Validation set length: 55
Test set length: 110
--------------------------------------------------
Retrieving parameters GPT2 model size 124M
Finished retrieving
Loading weights into GPT model
Initial losses
Training loss: 4.203367042541504
Validation loss: 4.050935935974121
Ep 1 (Step 000000): Batch train loss 4.270
Ep 1 (Step 000000): Train loss 3.223, Val loss 3.174
Ep 1 (Step 000001): Batch train loss 3.696
Ep 1 (Step 000002): Batch train loss 2.937
Ep 1 (Step 000003): Batch train loss 2.707
Ep 1 (Step 000004): Batch train loss 2.597
Ep 1 (Step 000005): Batch train loss 2.024
Ep 1 (Step 000005): Train loss 1.765, Val loss 1.796
Ep 1 (Step 000006): Batch train loss 1.971
Ep 1 (Step 000007): Batch train loss 1.754
Ep 1 (Step 000008): Batch train loss 1.738
Ep 1 (Step 000009): Batch train loss 1.766
Ep 1 (Step 000010): Batch train loss 1.616
Ep 1 (Step 000010): Train loss 1.355, Val loss 1.275
Ep 1 (Step 000011): Batch train loss 1.403
Ep 1 (Step 000012): Batch train loss 1.331
Ep 1 (Step 000013): Batch train loss 1.442
Ep 1 (Step 000014): Batch train loss 1.518
Ep 1 (Step 000015): Batch train loss 1.408
Ep 1 (Step 000015): Train loss 1.161, Val loss 1.168
Ep 1 (Step 000016): Batch train loss 1.326
Ep 1 (Step 000017): Batch train loss 1.359
Ep 1 (Step 000018): Batch train loss 1.266
Ep 1 (Step 000019): Batch train loss 1.025
Ep 1 (Step 000020): Batch train loss 1.316
Ep 1 (Step 000020): Train loss 0.942, Val loss 1.095
Ep 1 (Step 000021): Batch train loss 1.193
Ep 1 (Step 000022): Batch train loss 0.982
Ep 1 (Step 000023): Batch train loss 1.015
Ep 1 (Step 000024): Batch train loss 1.053
Ep 1 (Step 000025): Batch train loss 1.069
Ep 1 (Step 000025): Train loss 0.946, Val loss 1.050
Ep 1 (Step 000026): Batch train loss 1.368
Ep 1 (Step 000027): Batch train loss 1.233
Ep 1 (Step 000028): Batch train loss 1.114
Ep 1 (Step 000029): Batch train loss 0.991
Ep 1 (Step 000030): Batch train loss 1.069
Ep 1 (Step 000030): Train loss 0.963, Val loss 1.024
Ep 1 (Step 000031): Batch train loss 1.214
Ep 1 (Step 000032): Batch train loss 1.285
Ep 1 (Step 000033): Batch train loss 1.027
Ep 1 (Step 000034): Batch train loss 0.797
Ep 1 (Step 000035): Batch train loss 1.068
Ep 1 (Step 000035): Train loss 0.851, Val loss 1.001
Ep 1 (Step 000036): Batch train loss 1.108
Ep 1 (Step 000037): Batch train loss 0.890
Ep 1 (Step 000038): Batch train loss 0.965
Ep 1 (Step 000039): Batch train loss 0.758
Ep 1 (Step 000040): Batch train loss 1.308
Ep 1 (Step 000040): Train loss 0.941, Val loss 0.975
Ep 1 (Step 000041): Batch train loss 1.072
Ep 1 (Step 000042): Batch train loss 1.038
Ep 1 (Step 000043): Batch train loss 1.125
Ep 1 (Step 000044): Batch train loss 0.954
Ep 1 (Step 000045): Batch train loss 1.111
Ep 1 (Step 000045): Train loss 0.831, Val loss 0.950
Ep 1 (Step 000046): Batch train loss 1.025
Ep 1 (Step 000047): Batch train loss 0.831
Ep 1 (Step 000048): Batch train loss 0.828
Ep 1 (Step 000049): Batch train loss 0.972
Ep 1 (Step 000050): Batch train loss 0.835
Ep 1 (Step 000050): Train loss 0.877, Val loss 0.940
Ep 1 (Step 000051): Batch train loss 0.670
Ep 1 (Step 000052): Batch train loss 0.741
Ep 1 (Step 000053): Batch train loss 0.877
Ep 1 (Step 000054): Batch train loss 0.920
Ep 1 (Step 000055): Batch train loss 1.126
Ep 1 (Step 000055): Train loss 0.779, Val loss 0.925
Ep 1 (Step 000056): Batch train loss 1.169
Ep 1 (Step 000057): Batch train loss 0.923
Ep 1 (Step 000058): Batch train loss 0.971
Ep 1 (Step 000059): Batch train loss 0.855
Ep 1 (Step 000060): Batch train loss 0.833
Ep 1 (Step 000060): Train loss 0.854, Val loss 0.914
Ep 1 (Step 000061): Batch train loss 0.920
Ep 1 (Step 000062): Batch train loss 0.894
Ep 1 (Step 000063): Batch train loss 0.763
Ep 1 (Step 000064): Batch train loss 1.177
Ep 1 (Step 000065): Batch train loss 0.986
Ep 1 (Step 000065): Train loss 0.771, Val loss 0.910
Ep 1 (Step 000066): Batch train loss 0.951
Ep 1 (Step 000067): Batch train loss 0.770
Ep 1 (Step 000068): Batch train loss 0.859
Ep 1 (Step 000069): Batch train loss 0.993
Ep 1 (Step 000070): Batch train loss 0.828
Ep 1 (Step 000070): Train loss 0.740, Val loss 0.899
Ep 1 (Step 000071): Batch train loss 0.944
Ep 1 (Step 000072): Batch train loss 0.673
Ep 1 (Step 000073): Batch train loss 1.250
Ep 1 (Step 000074): Batch train loss 1.049
Ep 1 (Step 000075): Batch train loss 1.088
Ep 1 (Step 000075): Train loss 0.736, Val loss 0.888
Ep 1 (Step 000076): Batch train loss 0.832
Ep 1 (Step 000077): Batch train loss 1.041
Ep 1 (Step 000078): Batch train loss 1.210
Ep 1 (Step 000079): Batch train loss 0.805
Ep 1 (Step 000080): Batch train loss 0.892
Ep 1 (Step 000080): Train loss 0.799, Val loss 0.880
Ep 1 (Step 000081): Batch train loss 0.928
Ep 1 (Step 000082): Batch train loss 0.854
Ep 1 (Step 000083): Batch train loss 1.086
Ep 1 (Step 000084): Batch train loss 0.792
Ep 1 (Step 000085): Batch train loss 0.909
Ep 1 (Step 000085): Train loss 0.777, Val loss 0.876
Ep 1 (Step 000086): Batch train loss 0.889
Ep 1 (Step 000087): Batch train loss 0.878
Ep 1 (Step 000088): Batch train loss 0.993
Ep 1 (Step 000089): Batch train loss 1.077
Ep 1 (Step 000090): Batch train loss 0.966
Ep 1 (Step 000090): Train loss 0.695, Val loss 0.868
Ep 1 (Step 000091): Batch train loss 1.215
Ep 1 (Step 000092): Batch train loss 1.057
Ep 1 (Step 000093): Batch train loss 1.016
Ep 1 (Step 000094): Batch train loss 1.114
Ep 1 (Step 000095): Batch train loss 0.833
Ep 1 (Step 000095): Train loss 0.759, Val loss 0.867
Ep 1 (Step 000096): Batch train loss 0.992
Ep 1 (Step 000097): Batch train loss 0.741
Ep 1 (Step 000098): Batch train loss 0.933
Ep 1 (Step 000099): Batch train loss 0.980
Ep 1 (Step 000100): Batch train loss 0.815
Ep 1 (Step 000100): Train loss 0.677, Val loss 0.862
Ep 1 (Step 000101): Batch train loss 0.926
Ep 1 (Step 000102): Batch train loss 0.680
Ep 1 (Step 000103): Batch train loss 1.028
Ep 1 (Step 000104): Batch train loss 1.030
Ep 1 (Step 000105): Batch train loss 0.767
Ep 1 (Step 000105): Train loss 0.841, Val loss 0.859
Ep 1 (Step 000106): Batch train loss 1.080
Ep 1 (Step 000107): Batch train loss 0.985
Ep 1 (Step 000108): Batch train loss 0.895
Ep 1 (Step 000109): Batch train loss 0.794
Ep 1 (Step 000110): Batch train loss 0.975
Ep 1 (Step 000110): Train loss 0.736, Val loss 0.839
Ep 1 (Step 000111): Batch train loss 0.832
Ep 1 (Step 000112): Batch train loss 0.931
Ep 1 (Step 000113): Batch train loss 1.031
Ep 1 (Step 000114): Batch train loss 0.933
Ep 1 (Step 000115): Batch train loss 0.935
Ep 1 (Step 000115): Train loss 0.707, Val loss 0.837
Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the United States? 
Ep 2 (Step 000116): Batch train loss 0.788
Ep 2 (Step 000117): Batch train loss 0.752
Ep 2 (Step 000118): Batch train loss 0.917
Ep 2 (Step 000119): Batch train loss 0.711
Ep 2 (Step 000120): Batch train loss 0.846
Ep 2 (Step 000120): Train loss 0.731, Val loss 0.834
Ep 2 (Step 000121): Batch train loss 0.780
Ep 2 (Step 000122): Batch train loss 0.718
Ep 2 (Step 000123): Batch train loss 0.829
Ep 2 (Step 000124): Batch train loss 0.887
Ep 2 (Step 000125): Batch train loss 0.999
Ep 2 (Step 000125): Train loss 0.674, Val loss 0.828
Ep 2 (Step 000126): Batch train loss 0.719
Ep 2 (Step 000127): Batch train loss 0.831
Ep 2 (Step 000128): Batch train loss 0.625
Ep 2 (Step 000129): Batch train loss 0.754
Ep 2 (Step 000130): Batch train loss 0.884
Ep 2 (Step 000130): Train loss 0.639, Val loss 0.823
Ep 2 (Step 000131): Batch train loss 0.761
Ep 2 (Step 000132): Batch train loss 0.706
Ep 2 (Step 000133): Batch train loss 0.964
Ep 2 (Step 000134): Batch train loss 0.695
Ep 2 (Step 000135): Batch train loss 0.748
Ep 2 (Step 000135): Train loss 0.666, Val loss 0.817
Ep 2 (Step 000136): Batch train loss 0.792
Ep 2 (Step 000137): Batch train loss 0.466
Ep 2 (Step 000138): Batch train loss 0.916
Ep 2 (Step 000139): Batch train loss 0.786
Ep 2 (Step 000140): Batch train loss 0.911
Ep 2 (Step 000140): Train loss 0.680, Val loss 0.816
Ep 2 (Step 000141): Batch train loss 0.705
Ep 2 (Step 000142): Batch train loss 0.670
Ep 2 (Step 000143): Batch train loss 0.668
Ep 2 (Step 000144): Batch train loss 0.799
Ep 2 (Step 000145): Batch train loss 0.600
Ep 2 (Step 000145): Train loss 0.581, Val loss 0.811
Ep 2 (Step 000146): Batch train loss 0.642
Ep 2 (Step 000147): Batch train loss 0.734
Ep 2 (Step 000148): Batch train loss 0.655
Ep 2 (Step 000149): Batch train loss 0.816
Ep 2 (Step 000150): Batch train loss 0.630
Ep 2 (Step 000150): Train loss 0.683, Val loss 0.803
Ep 2 (Step 000151): Batch train loss 0.633
Ep 2 (Step 000152): Batch train loss 0.587
Ep 2 (Step 000153): Batch train loss 0.727
Ep 2 (Step 000154): Batch train loss 0.890
Ep 2 (Step 000155): Batch train loss 0.837
Ep 2 (Step 000155): Train loss 0.633, Val loss 0.802
Ep 2 (Step 000156): Batch train loss 0.734
Ep 2 (Step 000157): Batch train loss 0.539
Ep 2 (Step 000158): Batch train loss 0.834
Ep 2 (Step 000159): Batch train loss 0.801
Ep 2 (Step 000160): Batch train loss 0.568
Ep 2 (Step 000160): Train loss 0.586, Val loss 0.801
Ep 2 (Step 000161): Batch train loss 0.631
Ep 2 (Step 000162): Batch train loss 1.026
Ep 2 (Step 000163): Batch train loss 0.994
Ep 2 (Step 000164): Batch train loss 0.745
Ep 2 (Step 000165): Batch train loss 0.771
Ep 2 (Step 000165): Train loss 0.673, Val loss 0.802
Ep 2 (Step 000166): Batch train loss 0.736
Ep 2 (Step 000167): Batch train loss 0.536
Ep 2 (Step 000168): Batch train loss 0.597
Ep 2 (Step 000169): Batch train loss 0.796
Ep 2 (Step 000170): Batch train loss 0.467
Ep 2 (Step 000170): Train loss 0.643, Val loss 0.808
Ep 2 (Step 000171): Batch train loss 0.714
Ep 2 (Step 000172): Batch train loss 0.655
Ep 2 (Step 000173): Batch train loss 0.763
Ep 2 (Step 000174): Batch train loss 0.892
Ep 2 (Step 000175): Batch train loss 0.680
Ep 2 (Step 000175): Train loss 0.561, Val loss 0.808
Ep 2 (Step 000176): Batch train loss 0.771
Ep 2 (Step 000177): Batch train loss 0.721
Ep 2 (Step 000178): Batch train loss 0.533
Ep 2 (Step 000179): Batch train loss 0.701
Ep 2 (Step 000180): Batch train loss 0.580
Ep 2 (Step 000180): Train loss 0.627, Val loss 0.806
Ep 2 (Step 000181): Batch train loss 0.680
Ep 2 (Step 000182): Batch train loss 0.823
Ep 2 (Step 000183): Batch train loss 0.806
Ep 2 (Step 000184): Batch train loss 0.860
Ep 2 (Step 000185): Batch train loss 0.557
Ep 2 (Step 000185): Train loss 0.585, Val loss 0.800
Ep 2 (Step 000186): Batch train loss 0.772
Ep 2 (Step 000187): Batch train loss 0.740
Ep 2 (Step 000188): Batch train loss 0.697
Ep 2 (Step 000189): Batch train loss 0.663
Ep 2 (Step 000190): Batch train loss 0.632
Ep 2 (Step 000190): Train loss 0.581, Val loss 0.792
Ep 2 (Step 000191): Batch train loss 0.625
Ep 2 (Step 000192): Batch train loss 0.835
Ep 2 (Step 000193): Batch train loss 0.635
Ep 2 (Step 000194): Batch train loss 0.607
Ep 2 (Step 000195): Batch train loss 0.710
Ep 2 (Step 000195): Train loss 0.582, Val loss 0.785
Ep 2 (Step 000196): Batch train loss 0.574
Ep 2 (Step 000197): Batch train loss 0.699
Ep 2 (Step 000198): Batch train loss 0.609
Ep 2 (Step 000199): Batch train loss 0.799
Ep 2 (Step 000200): Batch train loss 0.740
Ep 2 (Step 000200): Train loss 0.571, Val loss 0.778
Ep 2 (Step 000201): Batch train loss 0.928
Ep 2 (Step 000202): Batch train loss 0.787
Ep 2 (Step 000203): Batch train loss 0.731
Ep 2 (Step 000204): Batch train loss 0.773
Ep 2 (Step 000205): Batch train loss 0.839
Ep 2 (Step 000205): Train loss 0.586, Val loss 0.773
Ep 2 (Step 000206): Batch train loss 0.601
Ep 2 (Step 000207): Batch train loss 0.949
Ep 2 (Step 000208): Batch train loss 0.767
Ep 2 (Step 000209): Batch train loss 0.691
Ep 2 (Step 000210): Batch train loss 0.792
Ep 2 (Step 000210): Train loss 0.537, Val loss 0.771
Ep 2 (Step 000211): Batch train loss 0.399
Ep 2 (Step 000212): Batch train loss 0.732
Ep 2 (Step 000213): Batch train loss 0.603
Ep 2 (Step 000214): Batch train loss 0.945
Ep 2 (Step 000215): Batch train loss 0.844
Ep 2 (Step 000215): Train loss 0.613, Val loss 0.771
Ep 2 (Step 000216): Batch train loss 0.862
Ep 2 (Step 000217): Batch train loss 0.802
Ep 2 (Step 000218): Batch train loss 0.812
Ep 2 (Step 000219): Batch train loss 0.734
Ep 2 (Step 000220): Batch train loss 0.691
Ep 2 (Step 000220): Train loss 0.549, Val loss 0.769
Ep 2 (Step 000221): Batch train loss 0.696
Ep 2 (Step 000222): Batch train loss 0.724
Ep 2 (Step 000223): Batch train loss 0.534
Ep 2 (Step 000224): Batch train loss 0.479
Ep 2 (Step 000225): Batch train loss 0.661
Ep 2 (Step 000225): Train loss 0.609, Val loss 0.767
Ep 2 (Step 000226): Batch train loss 0.563
Ep 2 (Step 000227): Batch train loss 0.656
Ep 2 (Step 000228): Batch train loss 0.704
Ep 2 (Step 000229): Batch train loss 0.458
Ep 2 (Step 000230): Batch train loss 0.706
Ep 2 (Step 000230): Train loss 0.541, Val loss 0.764
Ep 2 (Step 000231): Batch train loss 0.669
Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The chef prepared the meal every day.  
Training completed in 0.31 minutes.
Plot saved as ../output_dir/model_instruct_llm/loss-plot.png
--------------------------------------------------
Model saved as ../output_dir/model_instruct_llm/model_instruct_llm.pth
Maximum GPU memory allocated: 3.52 GB
Generating responses
  0%|          | 0/110 [00:00<?, ?it/s]  1%|          | 1/110 [00:00<00:17,  6.34it/s]  2%|▏         | 2/110 [00:00<00:22,  4.82it/s]  3%|▎         | 3/110 [00:00<00:24,  4.38it/s]  4%|▎         | 4/110 [00:00<00:21,  4.85it/s]  5%|▍         | 5/110 [00:01<00:20,  5.04it/s]  5%|▌         | 6/110 [00:01<00:20,  5.17it/s]  6%|▋         | 7/110 [00:01<00:20,  5.05it/s]  7%|▋         | 8/110 [00:01<00:19,  5.26it/s]  8%|▊         | 9/110 [00:01<00:21,  4.68it/s]  9%|▉         | 10/110 [00:02<00:21,  4.65it/s] 10%|█         | 11/110 [00:02<00:20,  4.83it/s] 11%|█         | 12/110 [00:02<00:20,  4.83it/s] 12%|█▏        | 13/110 [00:02<00:18,  5.27it/s] 13%|█▎        | 14/110 [00:02<00:17,  5.43it/s] 14%|█▎        | 15/110 [00:02<00:17,  5.33it/s] 15%|█▍        | 16/110 [00:03<00:16,  5.68it/s] 15%|█▌        | 17/110 [00:03<00:16,  5.76it/s] 16%|█▋        | 18/110 [00:03<00:14,  6.14it/s] 17%|█▋        | 19/110 [00:03<00:16,  5.48it/s] 18%|█▊        | 20/110 [00:03<00:19,  4.69it/s] 19%|█▉        | 21/110 [00:04<00:17,  4.99it/s] 20%|██        | 22/110 [00:04<00:16,  5.23it/s] 21%|██        | 23/110 [00:04<00:16,  5.21it/s] 22%|██▏       | 24/110 [00:04<00:16,  5.08it/s] 23%|██▎       | 25/110 [00:04<00:19,  4.47it/s] 24%|██▎       | 26/110 [00:05<00:17,  4.73it/s] 25%|██▍       | 27/110 [00:05<00:22,  3.69it/s] 25%|██▌       | 28/110 [00:05<00:26,  3.14it/s] 26%|██▋       | 29/110 [00:06<00:22,  3.56it/s] 27%|██▋       | 30/110 [00:06<00:19,  4.16it/s] 28%|██▊       | 31/110 [00:06<00:18,  4.29it/s] 29%|██▉       | 32/110 [00:06<00:17,  4.46it/s] 30%|███       | 33/110 [00:07<00:20,  3.76it/s] 31%|███       | 34/110 [00:07<00:18,  4.21it/s] 32%|███▏      | 35/110 [00:07<00:16,  4.46it/s] 33%|███▎      | 36/110 [00:07<00:16,  4.52it/s] 34%|███▎      | 37/110 [00:07<00:15,  4.70it/s] 35%|███▍      | 38/110 [00:08<00:14,  5.09it/s] 35%|███▌      | 39/110 [00:08<00:14,  5.03it/s] 36%|███▋      | 40/110 [00:08<00:14,  4.67it/s] 37%|███▋      | 41/110 [00:08<00:15,  4.52it/s] 38%|███▊      | 42/110 [00:08<00:15,  4.48it/s] 39%|███▉      | 43/110 [00:09<00:18,  3.59it/s] 40%|████      | 44/110 [00:09<00:16,  3.89it/s] 41%|████      | 45/110 [00:09<00:13,  4.67it/s] 42%|████▏     | 46/110 [00:09<00:12,  5.05it/s] 43%|████▎     | 47/110 [00:10<00:12,  5.17it/s] 44%|████▎     | 48/110 [00:10<00:11,  5.35it/s] 45%|████▍     | 49/110 [00:10<00:11,  5.29it/s] 45%|████▌     | 50/110 [00:10<00:12,  4.82it/s] 46%|████▋     | 51/110 [00:10<00:12,  4.67it/s] 47%|████▋     | 52/110 [00:11<00:11,  5.13it/s] 48%|████▊     | 53/110 [00:11<00:11,  4.96it/s] 49%|████▉     | 54/110 [00:11<00:12,  4.62it/s] 50%|█████     | 55/110 [00:11<00:12,  4.29it/s] 51%|█████     | 56/110 [00:12<00:14,  3.72it/s] 52%|█████▏    | 57/110 [00:12<00:13,  4.06it/s] 53%|█████▎    | 58/110 [00:12<00:11,  4.61it/s] 54%|█████▎    | 59/110 [00:12<00:11,  4.33it/s] 55%|█████▍    | 60/110 [00:12<00:11,  4.34it/s] 55%|█████▌    | 61/110 [00:13<00:11,  4.42it/s] 56%|█████▋    | 62/110 [00:13<00:11,  4.35it/s] 57%|█████▋    | 63/110 [00:13<00:10,  4.49it/s] 58%|█████▊    | 64/110 [00:13<00:09,  4.60it/s] 59%|█████▉    | 65/110 [00:14<00:10,  4.47it/s] 60%|██████    | 66/110 [00:14<00:09,  4.45it/s] 61%|██████    | 67/110 [00:14<00:08,  4.96it/s] 62%|██████▏   | 68/110 [00:14<00:07,  5.30it/s] 63%|██████▎   | 69/110 [00:14<00:07,  5.36it/s] 64%|██████▎   | 70/110 [00:15<00:07,  5.03it/s] 65%|██████▍   | 71/110 [00:15<00:08,  4.74it/s] 65%|██████▌   | 72/110 [00:15<00:08,  4.30it/s] 66%|██████▋   | 73/110 [00:15<00:08,  4.34it/s] 67%|██████▋   | 74/110 [00:15<00:08,  4.49it/s] 68%|██████▊   | 75/110 [00:18<00:36,  1.05s/it] 69%|██████▉   | 76/110 [00:19<00:26,  1.27it/s] 70%|███████   | 77/110 [00:19<00:21,  1.51it/s] 71%|███████   | 78/110 [00:19<00:18,  1.70it/s] 72%|███████▏  | 79/110 [00:20<00:16,  1.91it/s] 73%|███████▎  | 80/110 [00:20<00:13,  2.26it/s] 74%|███████▎  | 81/110 [00:20<00:10,  2.68it/s] 75%|███████▍  | 82/110 [00:20<00:09,  3.07it/s] 75%|███████▌  | 83/110 [00:21<00:08,  3.16it/s] 76%|███████▋  | 84/110 [00:21<00:07,  3.62it/s] 77%|███████▋  | 85/110 [00:21<00:06,  3.77it/s] 78%|███████▊  | 86/110 [00:21<00:06,  3.99it/s] 79%|███████▉  | 87/110 [00:22<00:05,  4.35it/s] 80%|████████  | 88/110 [00:22<00:05,  3.86it/s] 81%|████████  | 89/110 [00:22<00:05,  4.06it/s] 82%|████████▏ | 90/110 [00:22<00:04,  4.28it/s] 83%|████████▎ | 91/110 [00:22<00:04,  4.74it/s] 84%|████████▎ | 92/110 [00:23<00:03,  4.72it/s] 85%|████████▍ | 93/110 [00:23<00:03,  4.42it/s] 85%|████████▌ | 94/110 [00:23<00:03,  4.42it/s] 86%|████████▋ | 95/110 [00:23<00:03,  4.56it/s] 87%|████████▋ | 96/110 [00:24<00:03,  4.32it/s] 88%|████████▊ | 97/110 [00:24<00:02,  4.85it/s] 89%|████████▉ | 98/110 [00:24<00:02,  5.03it/s] 90%|█████████ | 99/110 [00:24<00:02,  5.09it/s] 91%|█████████ | 100/110 [00:24<00:02,  4.57it/s] 92%|█████████▏| 101/110 [00:25<00:01,  5.16it/s] 93%|█████████▎| 102/110 [00:25<00:01,  4.99it/s] 94%|█████████▎| 103/110 [00:25<00:01,  3.66it/s] 95%|█████████▍| 104/110 [00:26<00:01,  3.63it/s] 95%|█████████▌| 105/110 [00:28<00:05,  1.08s/it] 96%|█████████▋| 106/110 [00:29<00:03,  1.20it/s] 97%|█████████▋| 107/110 [00:29<00:01,  1.56it/s] 98%|█████████▊| 108/110 [00:29<00:00,  2.01it/s] 99%|█████████▉| 109/110 [00:29<00:00,  2.59it/s]100%|██████████| 110/110 [00:29<00:00,  3.05it/s]100%|██████████| 110/110 [00:29<00:00,  3.68it/s]
Responses saved as ../output_dir/model_instruct_llm/test_set_results.json
